Proceeding with code...

Using config file: configs/shearnet/forklike/ideal_psf/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: fork-like_ideal_high-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train PSF images: (100000, 53, 53)
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/fork-like_ideal_high-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/fork-like_ideal_high-noise/architecture.py
Epoch 1/300
Validation Loss: 1.9837e-02
New best validation loss: 1.9837e-02
Epoch 2/300
Validation Loss: 1.4188e-02
New best validation loss: 1.4188e-02
Epoch 3/300
Validation Loss: 1.4258e-02
No improvement in validation loss. Patience: 1/25
Epoch 4/300
Validation Loss: 1.1791e-02
New best validation loss: 1.1791e-02
Epoch 5/300
Validation Loss: 1.4242e-02
No improvement in validation loss. Patience: 1/25
Epoch 6/300
Validation Loss: 1.1702e-02
New best validation loss: 1.1702e-02
Epoch 7/300
Validation Loss: 1.1936e-02
No improvement in validation loss. Patience: 1/25
Epoch 8/300
Validation Loss: 1.0365e-02
New best validation loss: 1.0365e-02
Epoch 9/300
Validation Loss: 1.1542e-02
No improvement in validation loss. Patience: 1/25
Epoch 10/300
Validation Loss: 1.0631e-02
No improvement in validation loss. Patience: 2/25
Epoch 11/300
Validation Loss: 1.0266e-02
New best validation loss: 1.0266e-02
Epoch 12/300
Validation Loss: 1.0358e-02
No improvement in validation loss. Patience: 1/25
Epoch 13/300
Validation Loss: 1.0030e-02
New best validation loss: 1.0030e-02
Epoch 14/300
Validation Loss: 1.0259e-02
No improvement in validation loss. Patience: 1/25
Epoch 15/300
Validation Loss: 1.2233e-02
No improvement in validation loss. Patience: 2/25
Epoch 16/300
Validation Loss: 9.9124e-03
New best validation loss: 9.9124e-03
Epoch 17/300
Validation Loss: 9.9728e-03
No improvement in validation loss. Patience: 1/25
Epoch 18/300
Validation Loss: 1.0422e-02
No improvement in validation loss. Patience: 2/25
Epoch 19/300
Validation Loss: 9.8417e-03
New best validation loss: 9.8417e-03
Epoch 20/300
Validation Loss: 9.9459e-03
No improvement in validation loss. Patience: 1/25
Epoch 21/300
Validation Loss: 1.0446e-02
No improvement in validation loss. Patience: 2/25
Epoch 22/300
Validation Loss: 1.0290e-02
No improvement in validation loss. Patience: 3/25
Epoch 23/300
Validation Loss: 9.6372e-03
New best validation loss: 9.6372e-03
Epoch 24/300
Validation Loss: 9.6649e-03
No improvement in validation loss. Patience: 1/25
Epoch 25/300
Validation Loss: 9.4856e-03
New best validation loss: 9.4856e-03
Epoch 26/300
Validation Loss: 9.3454e-03
New best validation loss: 9.3454e-03
Epoch 27/300
Validation Loss: 9.4319e-03
No improvement in validation loss. Patience: 1/25
Epoch 28/300
Validation Loss: 9.2932e-03
New best validation loss: 9.2932e-03
Epoch 29/300
Validation Loss: 1.0013e-02
No improvement in validation loss. Patience: 1/25
Epoch 30/300
Validation Loss: 9.3736e-03
No improvement in validation loss. Patience: 2/25
Epoch 31/300
Validation Loss: 1.0011e-02
No improvement in validation loss. Patience: 3/25
Epoch 32/300
Validation Loss: 1.0474e-02
No improvement in validation loss. Patience: 4/25
Epoch 33/300
Validation Loss: 1.2077e-02
No improvement in validation loss. Patience: 5/25
Epoch 34/300
Validation Loss: 9.4414e-03
No improvement in validation loss. Patience: 6/25
Epoch 35/300
Validation Loss: 9.6219e-03
No improvement in validation loss. Patience: 7/25
Epoch 36/300
Validation Loss: 1.0957e-02
No improvement in validation loss. Patience: 8/25
Epoch 37/300
Validation Loss: 1.0182e-02
No improvement in validation loss. Patience: 9/25
Epoch 38/300
Validation Loss: 1.1028e-02
No improvement in validation loss. Patience: 10/25
Epoch 39/300
Validation Loss: 1.0243e-02
No improvement in validation loss. Patience: 11/25
Epoch 40/300
Validation Loss: 9.4772e-03
No improvement in validation loss. Patience: 12/25
Epoch 41/300
Validation Loss: 9.1549e-03
New best validation loss: 9.1549e-03
Epoch 42/300
Validation Loss: 9.8609e-03
No improvement in validation loss. Patience: 1/25
Epoch 43/300
Validation Loss: 9.4186e-03
No improvement in validation loss. Patience: 2/25
Epoch 44/300
Validation Loss: 9.9714e-03
No improvement in validation loss. Patience: 3/25
Epoch 45/300
Validation Loss: 9.8896e-03
No improvement in validation loss. Patience: 4/25
Epoch 46/300
Validation Loss: 1.2095e-02
No improvement in validation loss. Patience: 5/25
Epoch 47/300
Validation Loss: 1.0218e-02
No improvement in validation loss. Patience: 6/25
Epoch 48/300
Validation Loss: 9.8151e-03
No improvement in validation loss. Patience: 7/25
Epoch 49/300
Validation Loss: 1.1355e-02
No improvement in validation loss. Patience: 8/25
Epoch 50/300
Validation Loss: 9.3209e-03
No improvement in validation loss. Patience: 9/25
Epoch 51/300
Validation Loss: 1.0763e-02
No improvement in validation loss. Patience: 10/25
Epoch 52/300
Validation Loss: 1.0070e-02
No improvement in validation loss. Patience: 11/25
Epoch 53/300
Validation Loss: 9.3517e-03
No improvement in validation loss. Patience: 12/25
Epoch 54/300
Validation Loss: 9.6152e-03
No improvement in validation loss. Patience: 13/25
Epoch 55/300
Validation Loss: 9.8976e-03
No improvement in validation loss. Patience: 14/25
Epoch 56/300
Validation Loss: 9.5000e-03
No improvement in validation loss. Patience: 15/25
Epoch 57/300
Validation Loss: 9.5771e-03
No improvement in validation loss. Patience: 16/25
Epoch 58/300
Validation Loss: 9.5787e-03
No improvement in validation loss. Patience: 17/25
Epoch 59/300
Validation Loss: 1.0311e-02
No improvement in validation loss. Patience: 18/25
Epoch 60/300
Validation Loss: 1.1866e-02
No improvement in validation loss. Patience: 19/25
Epoch 61/300
Validation Loss: 9.6703e-03
No improvement in validation loss. Patience: 20/25
Epoch 62/300
Validation Loss: 1.1916e-02
No improvement in validation loss. Patience: 21/25
Epoch 63/300
Validation Loss: 1.1188e-02
No improvement in validation loss. Patience: 22/25
Epoch 64/300
Validation Loss: 9.8011e-03
No improvement in validation loss. Patience: 23/25
Epoch 65/300
Validation Loss: 9.7935e-03
No improvement in validation loss. Patience: 24/25
Epoch 66/300
Validation Loss: 9.7015e-03
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 66.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/fork-like_ideal_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test galaxy images: (5000, 53, 53)
Shape of test PSF images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: fork-like_ideal_high-noise66
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m9.669912e-03[0m
Average Bias from ShearNet: [1m[93m-2.695867e-03[0m
Time taken: [1m[96m4.59 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 4.204222e-03, Bias = +3.967653e-03
             g2: MSE = 4.180079e-03, Bias = -1.519824e-03
  g1g2_combined: MSE = 4.192153e-03, Bias = +1.223914e-03
          sigma: MSE = 2.590153e-03, Bias = -1.054535e-02
           flux: MSE = 2.770519e-02, Bias = -2.685947e-03

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96
[NaN Filter] Removed 37 rows with NaNs in predictions.

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m2.357503e-02[0m
Average Bias from NGmix: [1m[93m1.977168e-02[0m
Time taken: [1m[96m108.12 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 7.553993e-03, Bias = +2.214919e-02
             g2: MSE = 6.097541e-03, Bias = +1.903020e-03
  g1g2_combined: MSE = 6.825767e-03, Bias = +1.202610e-02
          sigma: MSE = 2.021650e-02, Bias = +6.790907e-02
           flux: MSE = 6.043207e-02, Bias = -1.287455e-02


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 2.686987e+03
Average Bias from MOM: -1.024526e+00
Time taken: 23.45 seconds

=== Per-Label Metrics ===
             g1: MSE = 1.870241e+02, Bias = -3.059202e-01
             g2: MSE = 5.186950e+03, Bias = -1.743133e+00
  g1g2_combined: MSE = 2.686987e+03, Bias = -1.024526e+00

[NaN Filter] Removed 37 rows with NaNs in predictions.

Generating plots...
Plotting residuals...
Plotting galaxy samples...
Plotting psf samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/forklike/ideal_psf/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: fork-like_ideal_low-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train PSF images: (100000, 53, 53)
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/fork-like_ideal_low-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/fork-like_ideal_low-noise/architecture.py
Epoch 1/300
Validation Loss: 4.1738e-03
New best validation loss: 4.1738e-03
Epoch 2/300
Validation Loss: 4.5755e-03
No improvement in validation loss. Patience: 1/25
Epoch 3/300
Validation Loss: 9.4742e-04
New best validation loss: 9.4742e-04
Epoch 4/300
Validation Loss: 9.5614e-04
No improvement in validation loss. Patience: 1/25
Epoch 5/300
Validation Loss: 5.8108e-04
New best validation loss: 5.8108e-04
Epoch 6/300
Validation Loss: 6.1012e-04
No improvement in validation loss. Patience: 1/25
Epoch 7/300
Validation Loss: 4.3239e-04
New best validation loss: 4.3239e-04
Epoch 8/300
Validation Loss: 4.3985e-04
No improvement in validation loss. Patience: 1/25
Epoch 9/300
Validation Loss: 3.8986e-04
New best validation loss: 3.8986e-04
Epoch 10/300
Validation Loss: 3.0893e-04
New best validation loss: 3.0893e-04
Epoch 11/300
Validation Loss: 4.2054e-04
No improvement in validation loss. Patience: 1/25
Epoch 12/300
Validation Loss: 4.2002e-04
No improvement in validation loss. Patience: 2/25
Epoch 13/300
Validation Loss: 2.4594e-04
New best validation loss: 2.4594e-04
Epoch 14/300
Validation Loss: 2.2104e-04
New best validation loss: 2.2104e-04
Epoch 15/300
Validation Loss: 2.1048e-03
No improvement in validation loss. Patience: 1/25
Epoch 16/300
Validation Loss: 5.6608e-04
No improvement in validation loss. Patience: 2/25
Epoch 17/300
Validation Loss: 2.1603e-04
New best validation loss: 2.1603e-04
Epoch 18/300
Validation Loss: 1.7697e-04
New best validation loss: 1.7697e-04
Epoch 19/300
Validation Loss: 3.8195e-04
No improvement in validation loss. Patience: 1/25
Epoch 20/300
Validation Loss: 1.1279e-04
New best validation loss: 1.1279e-04
Epoch 21/300
Validation Loss: 2.2157e-04
No improvement in validation loss. Patience: 1/25
Epoch 22/300
Validation Loss: 1.0883e-04
New best validation loss: 1.0883e-04
Epoch 23/300
Validation Loss: 1.4752e-04
No improvement in validation loss. Patience: 1/25
Epoch 24/300
Validation Loss: 1.1852e-04
No improvement in validation loss. Patience: 2/25
Epoch 25/300
Validation Loss: 3.3565e-04
No improvement in validation loss. Patience: 3/25
Epoch 26/300
Validation Loss: 1.6650e-04
No improvement in validation loss. Patience: 4/25
Epoch 27/300
Validation Loss: 1.0526e-04
New best validation loss: 1.0526e-04
Epoch 28/300
Validation Loss: 1.0191e-04
New best validation loss: 1.0191e-04
Epoch 29/300
Validation Loss: 1.1649e-04
No improvement in validation loss. Patience: 1/25
Epoch 30/300
Validation Loss: 1.0832e-04
No improvement in validation loss. Patience: 2/25
Epoch 31/300
Validation Loss: 9.4730e-05
New best validation loss: 9.4730e-05
Epoch 32/300
Validation Loss: 5.0652e-03
No improvement in validation loss. Patience: 1/25
Epoch 33/300
Validation Loss: 8.4492e-05
New best validation loss: 8.4492e-05
Epoch 34/300
Validation Loss: 2.0537e-04
No improvement in validation loss. Patience: 1/25
Epoch 35/300
Validation Loss: 8.0861e-05
New best validation loss: 8.0861e-05
Epoch 36/300
Validation Loss: 1.2879e-04
No improvement in validation loss. Patience: 1/25
Epoch 37/300
Validation Loss: 2.7916e-04
No improvement in validation loss. Patience: 2/25
Epoch 38/300
Validation Loss: 6.4585e-05
New best validation loss: 6.4585e-05
Epoch 39/300
Validation Loss: 7.3149e-05
No improvement in validation loss. Patience: 1/25
Epoch 40/300
Validation Loss: 8.4631e-05
No improvement in validation loss. Patience: 2/25
Epoch 41/300
Validation Loss: 5.7125e-05
New best validation loss: 5.7125e-05
Epoch 42/300
Validation Loss: 5.7967e-05
No improvement in validation loss. Patience: 1/25
Epoch 43/300
Validation Loss: 1.6463e-04
No improvement in validation loss. Patience: 2/25
Epoch 44/300
Validation Loss: 5.6611e-05
New best validation loss: 5.6611e-05
Epoch 45/300
Validation Loss: 1.5975e-04
No improvement in validation loss. Patience: 1/25
Epoch 46/300
Validation Loss: 2.7242e-03
No improvement in validation loss. Patience: 2/25
Epoch 47/300
Validation Loss: 8.9001e-05
No improvement in validation loss. Patience: 3/25
Epoch 48/300
Validation Loss: 3.3927e-04
No improvement in validation loss. Patience: 4/25
Epoch 49/300
Validation Loss: 1.4323e-04
No improvement in validation loss. Patience: 5/25
Epoch 50/300
Validation Loss: 1.0079e-04
No improvement in validation loss. Patience: 6/25
Epoch 51/300
Validation Loss: 2.3303e-04
No improvement in validation loss. Patience: 7/25
Epoch 52/300
Validation Loss: 5.3178e-05
New best validation loss: 5.3178e-05
Epoch 53/300
Validation Loss: 4.8117e-05
New best validation loss: 4.8117e-05
Epoch 54/300
Validation Loss: 6.0642e-05
No improvement in validation loss. Patience: 1/25
Epoch 55/300
Validation Loss: 1.2627e-03
No improvement in validation loss. Patience: 2/25
Epoch 56/300
Validation Loss: 8.5574e-05
No improvement in validation loss. Patience: 3/25
Epoch 57/300
Validation Loss: 2.7573e-04
No improvement in validation loss. Patience: 4/25
Epoch 58/300
Validation Loss: 5.0249e-05
No improvement in validation loss. Patience: 5/25
Epoch 59/300
Validation Loss: 4.8050e-05
New best validation loss: 4.8050e-05
Epoch 60/300
Validation Loss: 1.1532e-03
No improvement in validation loss. Patience: 1/25
Epoch 61/300
Validation Loss: 3.5129e-05
New best validation loss: 3.5129e-05
Epoch 62/300
Validation Loss: 2.5663e-04
No improvement in validation loss. Patience: 1/25
Epoch 63/300
Validation Loss: 7.8117e-05
No improvement in validation loss. Patience: 2/25
Epoch 64/300
Validation Loss: 4.8117e-05
No improvement in validation loss. Patience: 3/25
Epoch 65/300
Validation Loss: 1.3516e-04
No improvement in validation loss. Patience: 4/25
Epoch 66/300
Validation Loss: 3.1949e-05
New best validation loss: 3.1949e-05
Epoch 67/300
Validation Loss: 5.4102e-05
No improvement in validation loss. Patience: 1/25
Epoch 68/300
Validation Loss: 5.5373e-05
No improvement in validation loss. Patience: 2/25
Epoch 69/300
Validation Loss: 8.8968e-05
No improvement in validation loss. Patience: 3/25
Epoch 70/300
Validation Loss: 3.6351e-05
No improvement in validation loss. Patience: 4/25
Epoch 71/300
Validation Loss: 5.4510e-05
No improvement in validation loss. Patience: 5/25
Epoch 72/300
Validation Loss: 3.5120e-04
No improvement in validation loss. Patience: 6/25
Epoch 73/300
Validation Loss: 3.6672e-05
No improvement in validation loss. Patience: 7/25
Epoch 74/300
Validation Loss: 7.1588e-05
No improvement in validation loss. Patience: 8/25
Epoch 75/300
Validation Loss: 5.7958e-05
No improvement in validation loss. Patience: 9/25
Epoch 76/300
Validation Loss: 3.6055e-04
No improvement in validation loss. Patience: 10/25
Epoch 77/300
Validation Loss: 4.0594e-05
No improvement in validation loss. Patience: 11/25
Epoch 78/300
Validation Loss: 5.9436e-05
No improvement in validation loss. Patience: 12/25
Epoch 79/300
Validation Loss: 3.3168e-04
No improvement in validation loss. Patience: 13/25
Epoch 80/300
Validation Loss: 3.4563e-05
No improvement in validation loss. Patience: 14/25
Epoch 81/300
Validation Loss: 3.6274e-05
No improvement in validation loss. Patience: 15/25
Epoch 82/300
Validation Loss: 3.4671e-05
No improvement in validation loss. Patience: 16/25
Epoch 83/300
Validation Loss: 5.5802e-05
No improvement in validation loss. Patience: 17/25
Epoch 84/300
Validation Loss: 1.0049e-04
No improvement in validation loss. Patience: 18/25
Epoch 85/300
Validation Loss: 3.0214e-05
New best validation loss: 3.0214e-05
Epoch 86/300
Validation Loss: 5.0442e-05
No improvement in validation loss. Patience: 1/25
Epoch 87/300
Validation Loss: 6.0670e-05
No improvement in validation loss. Patience: 2/25
Epoch 88/300
Validation Loss: 2.9281e-05
New best validation loss: 2.9281e-05
Epoch 89/300
Validation Loss: 7.2439e-04
No improvement in validation loss. Patience: 1/25
Epoch 90/300
Validation Loss: 2.8897e-05
New best validation loss: 2.8897e-05
Epoch 91/300
Validation Loss: 2.0169e-04
No improvement in validation loss. Patience: 1/25
Epoch 92/300
Validation Loss: 4.0222e-05
No improvement in validation loss. Patience: 2/25
Epoch 93/300
Validation Loss: 4.2477e-05
No improvement in validation loss. Patience: 3/25
Epoch 94/300
Validation Loss: 2.8099e-05
New best validation loss: 2.8099e-05
Epoch 95/300
Validation Loss: 3.5810e-05
No improvement in validation loss. Patience: 1/25
Epoch 96/300
Validation Loss: 4.7700e-05
No improvement in validation loss. Patience: 2/25
Epoch 97/300
Validation Loss: 1.3243e-04
No improvement in validation loss. Patience: 3/25
Epoch 98/300
Validation Loss: 3.0964e-05
No improvement in validation loss. Patience: 4/25
Epoch 99/300
Validation Loss: 3.4694e-05
No improvement in validation loss. Patience: 5/25
Epoch 100/300
Validation Loss: 7.9356e-05
No improvement in validation loss. Patience: 6/25
Epoch 101/300
Validation Loss: 3.3623e-05
No improvement in validation loss. Patience: 7/25
Epoch 102/300
Validation Loss: 3.6749e-05
No improvement in validation loss. Patience: 8/25
Epoch 103/300
Validation Loss: 2.8518e-05
No improvement in validation loss. Patience: 9/25
Epoch 104/300
Validation Loss: 3.8397e-05
No improvement in validation loss. Patience: 10/25
Epoch 105/300
Validation Loss: 2.5056e-05
New best validation loss: 2.5056e-05
Epoch 106/300
Validation Loss: 3.1683e-05
No improvement in validation loss. Patience: 1/25
Epoch 107/300
Validation Loss: 2.6528e-05
No improvement in validation loss. Patience: 2/25
Epoch 108/300
Validation Loss: 3.4369e-05
No improvement in validation loss. Patience: 3/25
Epoch 109/300
Validation Loss: 4.0786e-04
No improvement in validation loss. Patience: 4/25
Epoch 110/300
Validation Loss: 6.0419e-05
No improvement in validation loss. Patience: 5/25
Epoch 111/300
Validation Loss: 7.9587e-05
No improvement in validation loss. Patience: 6/25
Epoch 112/300
Validation Loss: 9.6228e-05
No improvement in validation loss. Patience: 7/25
Epoch 113/300
Validation Loss: 2.9193e-05
No improvement in validation loss. Patience: 8/25
Epoch 114/300
Validation Loss: 3.0012e-05
No improvement in validation loss. Patience: 9/25
Epoch 115/300
Validation Loss: 1.1493e-04
No improvement in validation loss. Patience: 10/25
Epoch 116/300
Validation Loss: 6.6716e-05
No improvement in validation loss. Patience: 11/25
Epoch 117/300
Validation Loss: 1.0693e-04
No improvement in validation loss. Patience: 12/25
Epoch 118/300
Validation Loss: 3.0574e-05
No improvement in validation loss. Patience: 13/25
Epoch 119/300
Validation Loss: 4.2330e-05
No improvement in validation loss. Patience: 14/25
Epoch 120/300
Validation Loss: 4.1278e-05
No improvement in validation loss. Patience: 15/25
Epoch 121/300
Validation Loss: 2.6614e-05
No improvement in validation loss. Patience: 16/25
Epoch 122/300
Validation Loss: 3.1653e-05
No improvement in validation loss. Patience: 17/25
Epoch 123/300
Validation Loss: 2.7135e-05
No improvement in validation loss. Patience: 18/25
Epoch 124/300
Validation Loss: 2.2508e-05
New best validation loss: 2.2508e-05
Epoch 125/300
Validation Loss: 1.2633e-04
No improvement in validation loss. Patience: 1/25
Epoch 126/300
Validation Loss: 3.9214e-05
No improvement in validation loss. Patience: 2/25
Epoch 127/300
Validation Loss: 3.0052e-05
No improvement in validation loss. Patience: 3/25
Epoch 128/300
Validation Loss: 3.7058e-05
No improvement in validation loss. Patience: 4/25
Epoch 129/300
Validation Loss: 2.5987e-05
No improvement in validation loss. Patience: 5/25
Epoch 130/300
Validation Loss: 2.8313e-05
No improvement in validation loss. Patience: 6/25
Epoch 131/300
Validation Loss: 5.3319e-05
No improvement in validation loss. Patience: 7/25
Epoch 132/300
Validation Loss: 4.8102e-05
No improvement in validation loss. Patience: 8/25
Epoch 133/300
Validation Loss: 2.6699e-05
No improvement in validation loss. Patience: 9/25
Epoch 134/300
Validation Loss: 8.9270e-05
No improvement in validation loss. Patience: 10/25
Epoch 135/300
Validation Loss: 1.2188e-04
No improvement in validation loss. Patience: 11/25
Epoch 136/300
Validation Loss: 5.0365e-04
No improvement in validation loss. Patience: 12/25
Epoch 137/300
Validation Loss: 1.4233e-04
No improvement in validation loss. Patience: 13/25
Epoch 138/300
Validation Loss: 2.1559e-05
New best validation loss: 2.1559e-05
Epoch 139/300
Validation Loss: 4.7872e-05
No improvement in validation loss. Patience: 1/25
Epoch 140/300
Validation Loss: 4.5828e-05
No improvement in validation loss. Patience: 2/25
Epoch 141/300
Validation Loss: 5.1322e-05
No improvement in validation loss. Patience: 3/25
Epoch 142/300
Validation Loss: 2.9811e-05
No improvement in validation loss. Patience: 4/25
Epoch 143/300
Validation Loss: 2.2500e-05
No improvement in validation loss. Patience: 5/25
Epoch 144/300
Validation Loss: 2.4800e-05
No improvement in validation loss. Patience: 6/25
Epoch 145/300
Validation Loss: 2.4873e-05
No improvement in validation loss. Patience: 7/25
Epoch 146/300
Validation Loss: 1.1758e-04
No improvement in validation loss. Patience: 8/25
Epoch 147/300
Validation Loss: 2.3282e-05
No improvement in validation loss. Patience: 9/25
Epoch 148/300
Validation Loss: 2.6234e-05
No improvement in validation loss. Patience: 10/25
Epoch 149/300
Validation Loss: 2.9679e-05
No improvement in validation loss. Patience: 11/25
Epoch 150/300
Validation Loss: 1.1766e-04
No improvement in validation loss. Patience: 12/25
Epoch 151/300
Validation Loss: 7.3643e-05
No improvement in validation loss. Patience: 13/25
Epoch 152/300
Validation Loss: 2.4028e-05
No improvement in validation loss. Patience: 14/25
Epoch 153/300
Validation Loss: 2.1866e-05
No improvement in validation loss. Patience: 15/25
Epoch 154/300
Validation Loss: 2.5580e-05
No improvement in validation loss. Patience: 16/25
Epoch 155/300
Validation Loss: 2.3269e-05
No improvement in validation loss. Patience: 17/25
Epoch 156/300
Validation Loss: 1.8264e-05
New best validation loss: 1.8264e-05
Epoch 157/300
Validation Loss: 3.1254e-05
No improvement in validation loss. Patience: 1/25
Epoch 158/300
Validation Loss: 2.7856e-05
No improvement in validation loss. Patience: 2/25
Epoch 159/300
Validation Loss: 2.6431e-05
No improvement in validation loss. Patience: 3/25
Epoch 160/300
Validation Loss: 3.2411e-05
No improvement in validation loss. Patience: 4/25
Epoch 161/300
Validation Loss: 2.3707e-05
No improvement in validation loss. Patience: 5/25
Epoch 162/300
Validation Loss: 2.1763e-05
No improvement in validation loss. Patience: 6/25
Epoch 163/300
Validation Loss: 2.2535e-05
No improvement in validation loss. Patience: 7/25
Epoch 164/300
Validation Loss: 2.0988e-05
No improvement in validation loss. Patience: 8/25
Epoch 165/300
Validation Loss: 1.8162e-05
New best validation loss: 1.8162e-05
Epoch 166/300
Validation Loss: 1.8810e-05
No improvement in validation loss. Patience: 1/25
Epoch 167/300
Validation Loss: 2.0617e-05
No improvement in validation loss. Patience: 2/25
Epoch 168/300
Validation Loss: 1.7707e-05
New best validation loss: 1.7707e-05
Epoch 169/300
Validation Loss: 1.6715e-05
New best validation loss: 1.6715e-05
Epoch 170/300
Validation Loss: 1.9229e-05
No improvement in validation loss. Patience: 1/25
Epoch 171/300
Validation Loss: 2.0364e-05
No improvement in validation loss. Patience: 2/25
Epoch 172/300
Validation Loss: 2.8945e-05
No improvement in validation loss. Patience: 3/25
Epoch 173/300
Validation Loss: 2.2094e-05
No improvement in validation loss. Patience: 4/25
Epoch 174/300
Validation Loss: 2.1704e-05
No improvement in validation loss. Patience: 5/25
Epoch 175/300
Validation Loss: 1.8261e-05
No improvement in validation loss. Patience: 6/25
Epoch 176/300
Validation Loss: 1.7836e-05
No improvement in validation loss. Patience: 7/25
Epoch 177/300
Validation Loss: 1.7908e-05
No improvement in validation loss. Patience: 8/25
Epoch 178/300
Validation Loss: 2.0519e-05
No improvement in validation loss. Patience: 9/25
Epoch 179/300
Validation Loss: 1.8383e-05
No improvement in validation loss. Patience: 10/25
Epoch 180/300
Validation Loss: 2.4509e-05
No improvement in validation loss. Patience: 11/25
Epoch 181/300
Validation Loss: 1.9813e-05
No improvement in validation loss. Patience: 12/25
Epoch 182/300
Validation Loss: 2.0209e-05
No improvement in validation loss. Patience: 13/25
Epoch 183/300
Validation Loss: 1.9380e-05
No improvement in validation loss. Patience: 14/25
Epoch 184/300
Validation Loss: 3.5274e-05
No improvement in validation loss. Patience: 15/25
Epoch 185/300
Validation Loss: 1.7402e-05
No improvement in validation loss. Patience: 16/25
Epoch 186/300
Validation Loss: 1.9574e-05
No improvement in validation loss. Patience: 17/25
Epoch 187/300
Validation Loss: 3.7213e-05
No improvement in validation loss. Patience: 18/25
Epoch 188/300
Validation Loss: 1.8272e-05
No improvement in validation loss. Patience: 19/25
Epoch 189/300
Validation Loss: 3.3570e-05
No improvement in validation loss. Patience: 20/25
Epoch 190/300
Validation Loss: 1.7714e-05
No improvement in validation loss. Patience: 21/25
Epoch 191/300
Validation Loss: 1.7371e-05
No improvement in validation loss. Patience: 22/25
Epoch 192/300
Validation Loss: 1.5723e-05
New best validation loss: 1.5723e-05
Epoch 193/300
Validation Loss: 2.1684e-05
No improvement in validation loss. Patience: 1/25
Epoch 194/300
Validation Loss: 1.9747e-05
No improvement in validation loss. Patience: 2/25
Epoch 195/300
Validation Loss: 1.7960e-05
No improvement in validation loss. Patience: 3/25
Epoch 196/300
Validation Loss: 1.8843e-05
No improvement in validation loss. Patience: 4/25
Epoch 197/300
Validation Loss: 2.0790e-05
No improvement in validation loss. Patience: 5/25
Epoch 198/300
Validation Loss: 7.0628e-05
No improvement in validation loss. Patience: 6/25
Epoch 199/300
Validation Loss: 1.6515e-05
No improvement in validation loss. Patience: 7/25
Epoch 200/300
Validation Loss: 1.4911e-05
New best validation loss: 1.4911e-05
Epoch 201/300
Validation Loss: 1.5302e-05
No improvement in validation loss. Patience: 1/25
Epoch 202/300
Validation Loss: 1.8684e-05
No improvement in validation loss. Patience: 2/25
Epoch 203/300
Validation Loss: 2.1266e-05
No improvement in validation loss. Patience: 3/25
Epoch 204/300
Validation Loss: 2.0176e-05
No improvement in validation loss. Patience: 4/25
Epoch 205/300
Validation Loss: 1.7631e-05
No improvement in validation loss. Patience: 5/25
Epoch 206/300
Validation Loss: 1.5739e-05
No improvement in validation loss. Patience: 6/25
Epoch 207/300
Validation Loss: 1.5936e-05
No improvement in validation loss. Patience: 7/25
Epoch 208/300
Validation Loss: 2.4187e-05
No improvement in validation loss. Patience: 8/25
Epoch 209/300
Validation Loss: 1.9617e-05
No improvement in validation loss. Patience: 9/25
Epoch 210/300
Validation Loss: 1.8868e-05
No improvement in validation loss. Patience: 10/25
Epoch 211/300
Validation Loss: 1.7587e-05
No improvement in validation loss. Patience: 11/25
Epoch 212/300
Validation Loss: 1.5405e-05
No improvement in validation loss. Patience: 12/25
Epoch 213/300
Validation Loss: 1.5697e-05
No improvement in validation loss. Patience: 13/25
Epoch 214/300
Validation Loss: 1.7379e-05
No improvement in validation loss. Patience: 14/25
Epoch 215/300
Validation Loss: 2.0090e-05
No improvement in validation loss. Patience: 15/25
Epoch 216/300
Validation Loss: 1.5707e-05
No improvement in validation loss. Patience: 16/25
Epoch 217/300
Validation Loss: 1.5558e-05
No improvement in validation loss. Patience: 17/25
Epoch 218/300
Validation Loss: 1.5057e-05
No improvement in validation loss. Patience: 18/25
Epoch 219/300
Validation Loss: 1.5847e-05
No improvement in validation loss. Patience: 19/25
Epoch 220/300
Validation Loss: 1.4481e-05
New best validation loss: 1.4481e-05
Epoch 221/300
Validation Loss: 1.5561e-05
No improvement in validation loss. Patience: 1/25
Epoch 222/300
Validation Loss: 1.6693e-05
No improvement in validation loss. Patience: 2/25
Epoch 223/300
Validation Loss: 1.4236e-05
New best validation loss: 1.4236e-05
Epoch 224/300
Validation Loss: 1.6695e-05
No improvement in validation loss. Patience: 1/25
Epoch 225/300
Validation Loss: 1.5605e-05
No improvement in validation loss. Patience: 2/25
Epoch 226/300
Validation Loss: 1.5060e-05
No improvement in validation loss. Patience: 3/25
Epoch 227/300
Validation Loss: 1.5915e-05
No improvement in validation loss. Patience: 4/25
Epoch 228/300
Validation Loss: 1.4201e-05
New best validation loss: 1.4201e-05
Epoch 229/300
Validation Loss: 1.4730e-05
No improvement in validation loss. Patience: 1/25
Epoch 230/300
Validation Loss: 1.7539e-05
No improvement in validation loss. Patience: 2/25
Epoch 231/300
Validation Loss: 1.4101e-05
New best validation loss: 1.4101e-05
Epoch 232/300
Validation Loss: 1.4129e-05
No improvement in validation loss. Patience: 1/25
Epoch 233/300
Validation Loss: 1.4238e-05
No improvement in validation loss. Patience: 2/25
Epoch 234/300
Validation Loss: 1.9260e-05
No improvement in validation loss. Patience: 3/25
Epoch 235/300
Validation Loss: 1.5032e-05
No improvement in validation loss. Patience: 4/25
Epoch 236/300
Validation Loss: 1.5478e-05
No improvement in validation loss. Patience: 5/25
Epoch 237/300
Validation Loss: 1.4888e-05
No improvement in validation loss. Patience: 6/25
Epoch 238/300
Validation Loss: 1.4032e-05
New best validation loss: 1.4032e-05
Epoch 239/300
Validation Loss: 1.5447e-05
No improvement in validation loss. Patience: 1/25
Epoch 240/300
Validation Loss: 1.4539e-05
No improvement in validation loss. Patience: 2/25
Epoch 241/300
Validation Loss: 1.5732e-05
No improvement in validation loss. Patience: 3/25
Epoch 242/300
Validation Loss: 1.5362e-05
No improvement in validation loss. Patience: 4/25
Epoch 243/300
Validation Loss: 1.5370e-05
No improvement in validation loss. Patience: 5/25
Epoch 244/300
Validation Loss: 1.4239e-05
No improvement in validation loss. Patience: 6/25
Epoch 245/300
Validation Loss: 1.5523e-05
No improvement in validation loss. Patience: 7/25
Epoch 246/300
Validation Loss: 1.4651e-05
No improvement in validation loss. Patience: 8/25
Epoch 247/300
Validation Loss: 1.3335e-05
New best validation loss: 1.3335e-05
Epoch 248/300
Validation Loss: 1.4536e-05
No improvement in validation loss. Patience: 1/25
Epoch 249/300
Validation Loss: 1.5008e-05
No improvement in validation loss. Patience: 2/25
Epoch 250/300
Validation Loss: 1.3791e-05
No improvement in validation loss. Patience: 3/25
Epoch 251/300
Validation Loss: 1.4056e-05
No improvement in validation loss. Patience: 4/25
Epoch 252/300
Validation Loss: 1.3718e-05
No improvement in validation loss. Patience: 5/25
Epoch 253/300
Validation Loss: 1.4464e-05
No improvement in validation loss. Patience: 6/25
Epoch 254/300
Validation Loss: 1.3622e-05
No improvement in validation loss. Patience: 7/25
Epoch 255/300
Validation Loss: 1.4305e-05
No improvement in validation loss. Patience: 8/25
Epoch 256/300
Validation Loss: 1.3450e-05
No improvement in validation loss. Patience: 9/25
Epoch 257/300
Validation Loss: 1.3231e-05
New best validation loss: 1.3231e-05
Epoch 258/300
Validation Loss: 1.5343e-05
No improvement in validation loss. Patience: 1/25
Epoch 259/300
Validation Loss: 1.3256e-05
No improvement in validation loss. Patience: 2/25
Epoch 260/300
Validation Loss: 1.3727e-05
No improvement in validation loss. Patience: 3/25
Epoch 261/300
Validation Loss: 1.3869e-05
No improvement in validation loss. Patience: 4/25
Epoch 262/300
Validation Loss: 1.3285e-05
No improvement in validation loss. Patience: 5/25
Epoch 263/300
Validation Loss: 1.3352e-05
No improvement in validation loss. Patience: 6/25
Epoch 264/300
Validation Loss: 1.3580e-05
No improvement in validation loss. Patience: 7/25
Epoch 265/300
Validation Loss: 1.3291e-05
No improvement in validation loss. Patience: 8/25
Epoch 266/300
Validation Loss: 1.3860e-05
No improvement in validation loss. Patience: 9/25
Epoch 267/300
Validation Loss: 1.3511e-05
No improvement in validation loss. Patience: 10/25
Epoch 268/300
Validation Loss: 1.3346e-05
No improvement in validation loss. Patience: 11/25
Epoch 269/300
Validation Loss: 1.3508e-05
No improvement in validation loss. Patience: 12/25
Epoch 270/300
Validation Loss: 1.3233e-05
No improvement in validation loss. Patience: 13/25
Epoch 271/300
Validation Loss: 1.3452e-05
No improvement in validation loss. Patience: 14/25
Epoch 272/300
Validation Loss: 1.3422e-05
No improvement in validation loss. Patience: 15/25
Epoch 273/300
Validation Loss: 1.3306e-05
No improvement in validation loss. Patience: 16/25
Epoch 274/300
Validation Loss: 1.3190e-05
New best validation loss: 1.3190e-05
Epoch 275/300
Validation Loss: 1.3038e-05
New best validation loss: 1.3038e-05
Epoch 276/300
Validation Loss: 1.3253e-05
No improvement in validation loss. Patience: 1/25
Epoch 277/300
Validation Loss: 1.3309e-05
No improvement in validation loss. Patience: 2/25
Epoch 278/300
Validation Loss: 1.3192e-05
No improvement in validation loss. Patience: 3/25
Epoch 279/300
Validation Loss: 1.3341e-05
No improvement in validation loss. Patience: 4/25
Epoch 280/300
Validation Loss: 1.3178e-05
No improvement in validation loss. Patience: 5/25
Epoch 281/300
Validation Loss: 1.3044e-05
No improvement in validation loss. Patience: 6/25
Epoch 282/300
Validation Loss: 1.3139e-05
No improvement in validation loss. Patience: 7/25
Epoch 283/300
Validation Loss: 1.3232e-05
No improvement in validation loss. Patience: 8/25
Epoch 284/300
Validation Loss: 1.3197e-05
No improvement in validation loss. Patience: 9/25
Epoch 285/300
Validation Loss: 1.3215e-05
No improvement in validation loss. Patience: 10/25
Epoch 286/300
Validation Loss: 1.3184e-05
No improvement in validation loss. Patience: 11/25
Epoch 287/300
Validation Loss: 1.3072e-05
No improvement in validation loss. Patience: 12/25
Epoch 288/300
Validation Loss: 1.3128e-05
No improvement in validation loss. Patience: 13/25
Epoch 289/300
Validation Loss: 1.3072e-05
No improvement in validation loss. Patience: 14/25
Epoch 290/300
Validation Loss: 1.3244e-05
No improvement in validation loss. Patience: 15/25
Epoch 291/300
Validation Loss: 1.3085e-05
No improvement in validation loss. Patience: 16/25
Epoch 292/300
Validation Loss: 1.3057e-05
No improvement in validation loss. Patience: 17/25
Epoch 293/300
Validation Loss: 1.3104e-05
No improvement in validation loss. Patience: 18/25
Epoch 294/300
Validation Loss: 1.3112e-05
No improvement in validation loss. Patience: 19/25
Epoch 295/300
Validation Loss: 1.3078e-05
No improvement in validation loss. Patience: 20/25
Epoch 296/300
Validation Loss: 1.3099e-05
No improvement in validation loss. Patience: 21/25
Epoch 297/300
Validation Loss: 1.3119e-05
No improvement in validation loss. Patience: 22/25
Epoch 298/300
Validation Loss: 1.3089e-05
No improvement in validation loss. Patience: 23/25
Epoch 299/300
Validation Loss: 1.3094e-05
No improvement in validation loss. Patience: 24/25
Epoch 300/300
Validation Loss: 1.3084e-05
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 300.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/fork-like_ideal_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test galaxy images: (5000, 53, 53)
Shape of test PSF images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: fork-like_ideal_low-noise300
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m1.005331e-05[0m
Average Bias from ShearNet: [1m[93m1.839682e-05[0m
Time taken: [1m[96m6.92 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 4.164086e-06, Bias = -1.830988e-05
             g2: MSE = 4.104009e-06, Bias = -1.316421e-05
  g1g2_combined: MSE = 4.134048e-06, Bias = -1.573705e-05
          sigma: MSE = 1.041999e-05, Bias = +1.444031e-04
           flux: MSE = 2.152517e-05, Bias = -3.934176e-05

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m3.820454e-03[0m
Average Bias from NGmix: [1m[93m2.025632e-02[0m
Time taken: [1m[96m106.97 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.622450e-04, Bias = -3.714774e-04
             g2: MSE = 1.138660e-04, Bias = -1.905210e-04
  g1g2_combined: MSE = 1.380555e-04, Bias = -2.809992e-04
          sigma: MSE = 1.481033e-02, Bias = +8.233002e-02
           flux: MSE = 1.953707e-04, Bias = -7.427501e-04


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 4.641008e-03
Average Bias from MOM: -1.569084e-03
Time taken: 23.29 seconds

=== Per-Label Metrics ===
             g1: MSE = 4.552417e-03, Bias = +1.008680e-03
             g2: MSE = 4.729599e-03, Bias = -4.146848e-03
  g1g2_combined: MSE = 4.641008e-03, Bias = -1.569084e-03


Generating plots...
Plotting residuals...
Plotting galaxy samples...
Plotting psf samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/forklike/superbit_psf/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: fork-like_superbit_high-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train PSF images: (100000, 53, 53)
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/fork-like_superbit_high-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/fork-like_superbit_high-noise/architecture.py
Epoch 1/300
Validation Loss: 2.5749e-02
New best validation loss: 2.5749e-02
Epoch 2/300
Validation Loss: 1.9414e-02
New best validation loss: 1.9414e-02
Epoch 3/300
Validation Loss: 1.6474e-02
New best validation loss: 1.6474e-02
Epoch 4/300
Validation Loss: 1.4777e-02
New best validation loss: 1.4777e-02
Epoch 5/300
Validation Loss: 1.7968e-02
No improvement in validation loss. Patience: 1/25
Epoch 6/300
Validation Loss: 1.4890e-02
No improvement in validation loss. Patience: 2/25
Epoch 7/300
Validation Loss: 1.3734e-02
New best validation loss: 1.3734e-02
Epoch 8/300
Validation Loss: 1.3759e-02
No improvement in validation loss. Patience: 1/25
Epoch 9/300
Validation Loss: 1.5745e-02
No improvement in validation loss. Patience: 2/25
Epoch 10/300
Validation Loss: 1.3899e-02
No improvement in validation loss. Patience: 3/25
Epoch 11/300
Validation Loss: 1.3531e-02
New best validation loss: 1.3531e-02
Epoch 12/300
Validation Loss: 1.3184e-02
New best validation loss: 1.3184e-02
Epoch 13/300
Validation Loss: 1.3072e-02
New best validation loss: 1.3072e-02
Epoch 14/300
Validation Loss: 1.4309e-02
No improvement in validation loss. Patience: 1/25
Epoch 15/300
Validation Loss: 1.3448e-02
No improvement in validation loss. Patience: 2/25
Epoch 16/300
Validation Loss: 1.3626e-02
No improvement in validation loss. Patience: 3/25
Epoch 17/300
Validation Loss: 1.3084e-02
No improvement in validation loss. Patience: 4/25
Epoch 18/300
Validation Loss: 1.5272e-02
No improvement in validation loss. Patience: 5/25
Epoch 19/300
Validation Loss: 1.2823e-02
New best validation loss: 1.2823e-02
Epoch 20/300
Validation Loss: 1.3042e-02
No improvement in validation loss. Patience: 1/25
Epoch 21/300
Validation Loss: 1.4819e-02
No improvement in validation loss. Patience: 2/25
Epoch 22/300
Validation Loss: 1.7679e-02
No improvement in validation loss. Patience: 3/25
Epoch 23/300
Validation Loss: 1.3089e-02
No improvement in validation loss. Patience: 4/25
Epoch 24/300
Validation Loss: 1.2962e-02
No improvement in validation loss. Patience: 5/25
Epoch 25/300
Validation Loss: 1.2572e-02
New best validation loss: 1.2572e-02
Epoch 26/300
Validation Loss: 1.2329e-02
New best validation loss: 1.2329e-02
Epoch 27/300
Validation Loss: 1.2336e-02
No improvement in validation loss. Patience: 1/25
Epoch 28/300
Validation Loss: 1.3231e-02
No improvement in validation loss. Patience: 2/25
Epoch 29/300
Validation Loss: 1.2500e-02
No improvement in validation loss. Patience: 3/25
Epoch 30/300
Validation Loss: 1.2657e-02
No improvement in validation loss. Patience: 4/25
Epoch 31/300
Validation Loss: 1.3123e-02
No improvement in validation loss. Patience: 5/25
Epoch 32/300
Validation Loss: 1.2345e-02
No improvement in validation loss. Patience: 6/25
Epoch 33/300
Validation Loss: 1.3479e-02
No improvement in validation loss. Patience: 7/25
Epoch 34/300
Validation Loss: 1.2415e-02
No improvement in validation loss. Patience: 8/25
Epoch 35/300
Validation Loss: 1.3849e-02
No improvement in validation loss. Patience: 9/25
Epoch 36/300
Validation Loss: 1.2232e-02
New best validation loss: 1.2232e-02
Epoch 37/300
Validation Loss: 1.3664e-02
No improvement in validation loss. Patience: 1/25
Epoch 38/300
Validation Loss: 1.3627e-02
No improvement in validation loss. Patience: 2/25
Epoch 39/300
Validation Loss: 1.2619e-02
No improvement in validation loss. Patience: 3/25
Epoch 40/300
Validation Loss: 1.2275e-02
No improvement in validation loss. Patience: 4/25
Epoch 41/300
Validation Loss: 1.2163e-02
New best validation loss: 1.2163e-02
Epoch 42/300
Validation Loss: 1.2470e-02
No improvement in validation loss. Patience: 1/25
Epoch 43/300
Validation Loss: 1.2109e-02
New best validation loss: 1.2109e-02
Epoch 44/300
Validation Loss: 1.2253e-02
No improvement in validation loss. Patience: 1/25
Epoch 45/300
Validation Loss: 1.2304e-02
No improvement in validation loss. Patience: 2/25
Epoch 46/300
Validation Loss: 1.4672e-02
No improvement in validation loss. Patience: 3/25
Epoch 47/300
Validation Loss: 1.2657e-02
No improvement in validation loss. Patience: 4/25
Epoch 48/300
Validation Loss: 1.2075e-02
New best validation loss: 1.2075e-02
Epoch 49/300
Validation Loss: 1.2549e-02
No improvement in validation loss. Patience: 1/25
Epoch 50/300
Validation Loss: 1.2637e-02
No improvement in validation loss. Patience: 2/25
Epoch 51/300
Validation Loss: 1.3293e-02
No improvement in validation loss. Patience: 3/25
Epoch 52/300
Validation Loss: 1.3825e-02
No improvement in validation loss. Patience: 4/25
Epoch 53/300
Validation Loss: 1.2654e-02
No improvement in validation loss. Patience: 5/25
Epoch 54/300
Validation Loss: 1.3462e-02
No improvement in validation loss. Patience: 6/25
Epoch 55/300
Validation Loss: 1.2670e-02
No improvement in validation loss. Patience: 7/25
Epoch 56/300
Validation Loss: 1.2599e-02
No improvement in validation loss. Patience: 8/25
Epoch 57/300
Validation Loss: 1.7537e-02
No improvement in validation loss. Patience: 9/25
Epoch 58/300
Validation Loss: 1.2547e-02
No improvement in validation loss. Patience: 10/25
Epoch 59/300
Validation Loss: 1.2616e-02
No improvement in validation loss. Patience: 11/25
Epoch 60/300
Validation Loss: 1.4053e-02
No improvement in validation loss. Patience: 12/25
Epoch 61/300
Validation Loss: 1.3246e-02
No improvement in validation loss. Patience: 13/25
Epoch 62/300
Validation Loss: 1.2659e-02
No improvement in validation loss. Patience: 14/25
Epoch 63/300
Validation Loss: 1.3488e-02
No improvement in validation loss. Patience: 15/25
Epoch 64/300
Validation Loss: 1.3477e-02
No improvement in validation loss. Patience: 16/25
Epoch 65/300
Validation Loss: 1.2950e-02
No improvement in validation loss. Patience: 17/25
Epoch 66/300
Validation Loss: 1.2725e-02
No improvement in validation loss. Patience: 18/25
Epoch 67/300
Validation Loss: 1.4221e-02
No improvement in validation loss. Patience: 19/25
Epoch 68/300
Validation Loss: 1.6483e-02
No improvement in validation loss. Patience: 20/25
Epoch 69/300
Validation Loss: 1.3401e-02
No improvement in validation loss. Patience: 21/25
Epoch 70/300
Validation Loss: 1.3926e-02
No improvement in validation loss. Patience: 22/25
Epoch 71/300
Validation Loss: 1.3483e-02
No improvement in validation loss. Patience: 23/25
Epoch 72/300
Validation Loss: 1.3709e-02
No improvement in validation loss. Patience: 24/25
Epoch 73/300
Validation Loss: 1.3067e-02
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 73.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/fork-like_superbit_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test galaxy images: (5000, 53, 53)
Shape of test PSF images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: fork-like_superbit_high-noise73
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m3.240636e-02[0m
Average Bias from ShearNet: [1m[93m-1.056087e-02[0m
Time taken: [1m[96m6.85 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 3.557158e-02, Bias = +8.186452e-02
             g2: MSE = 2.524637e-02, Bias = -2.683920e-02
  g1g2_combined: MSE = 3.040897e-02, Bias = +2.751267e-02
          sigma: MSE = 1.469542e-02, Bias = -9.771533e-02
           flux: MSE = 5.411213e-02, Bias = +4.465269e-04

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96
[NaN Filter] Removed 30 rows with NaNs in predictions.

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m5.388719e-02[0m
Average Bias from NGmix: [1m[93m-5.198148e-02[0m
Time taken: [1m[96m108.11 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.058866e-02, Bias = +2.574496e-02
             g2: MSE = 9.366099e-03, Bias = -1.692582e-03
  g1g2_combined: MSE = 9.977378e-03, Bias = +1.202619e-02
          sigma: MSE = 2.479200e-02, Bias = +1.196303e-01
           flux: MSE = 1.708020e-01, Bias = -3.516086e-01


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 6.749586e+02
Average Bias from MOM: 5.894016e-01
Time taken: 23.20 seconds

=== Per-Label Metrics ===
             g1: MSE = 2.241609e+02, Bias = +3.292911e-01
             g2: MSE = 1.125756e+03, Bias = +8.495122e-01
  g1g2_combined: MSE = 6.749586e+02, Bias = +5.894016e-01

[NaN Filter] Removed 30 rows with NaNs in predictions.

Generating plots...
Plotting residuals...
Plotting galaxy samples...
Plotting psf samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/forklike/superbit_psf/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: fork-like_superbit_low-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train PSF images: (100000, 53, 53)
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/fork-like_superbit_low-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/fork-like_superbit_low-noise/architecture.py
Epoch 1/300
Validation Loss: 6.0747e-03
New best validation loss: 6.0747e-03
Epoch 2/300
Validation Loss: 3.7918e-03
New best validation loss: 3.7918e-03
Epoch 3/300
Validation Loss: 1.7442e-03
New best validation loss: 1.7442e-03
Epoch 4/300
Validation Loss: 1.5719e-03
New best validation loss: 1.5719e-03
Epoch 5/300
Validation Loss: 1.1528e-03
New best validation loss: 1.1528e-03
Epoch 6/300
Validation Loss: 1.2541e-03
No improvement in validation loss. Patience: 1/25
Epoch 7/300
Validation Loss: 1.0576e-03
New best validation loss: 1.0576e-03
Epoch 8/300
Validation Loss: 9.1700e-04
New best validation loss: 9.1700e-04
Epoch 9/300
Validation Loss: 7.1899e-04
New best validation loss: 7.1899e-04
Epoch 10/300
Validation Loss: 7.0537e-04
New best validation loss: 7.0537e-04
Epoch 11/300
Validation Loss: 5.1414e-04
New best validation loss: 5.1414e-04
Epoch 12/300
Validation Loss: 5.5670e-04
No improvement in validation loss. Patience: 1/25
Epoch 13/300
Validation Loss: 4.7371e-04
New best validation loss: 4.7371e-04
Epoch 14/300
Validation Loss: 4.0154e-04
New best validation loss: 4.0154e-04
Epoch 15/300
Validation Loss: 4.0813e-04
No improvement in validation loss. Patience: 1/25
Epoch 16/300
Validation Loss: 1.8998e-03
No improvement in validation loss. Patience: 2/25
Epoch 17/300
Validation Loss: 7.1042e-04
No improvement in validation loss. Patience: 3/25
Epoch 18/300
Validation Loss: 4.0162e-04
No improvement in validation loss. Patience: 4/25
Epoch 19/300
Validation Loss: 1.8165e-03
No improvement in validation loss. Patience: 5/25
Epoch 20/300
Validation Loss: 2.8415e-04
New best validation loss: 2.8415e-04
Epoch 21/300
Validation Loss: 4.3470e-04
No improvement in validation loss. Patience: 1/25
Epoch 22/300
Validation Loss: 3.0209e-04
No improvement in validation loss. Patience: 2/25
Epoch 23/300
Validation Loss: 2.8544e-04
No improvement in validation loss. Patience: 3/25
Epoch 24/300
Validation Loss: 3.1985e-04
No improvement in validation loss. Patience: 4/25
Epoch 25/300
Validation Loss: 5.8753e-04
No improvement in validation loss. Patience: 5/25
Epoch 26/300
Validation Loss: 3.1903e-04
No improvement in validation loss. Patience: 6/25
Epoch 27/300
Validation Loss: 2.4493e-04
New best validation loss: 2.4493e-04
Epoch 28/300
Validation Loss: 3.5246e-04
No improvement in validation loss. Patience: 1/25
Epoch 29/300
Validation Loss: 3.7500e-04
No improvement in validation loss. Patience: 2/25
Epoch 30/300
Validation Loss: 3.9350e-04
No improvement in validation loss. Patience: 3/25
Epoch 31/300
Validation Loss: 3.4140e-04
No improvement in validation loss. Patience: 4/25
Epoch 32/300
Validation Loss: 6.7097e-04
No improvement in validation loss. Patience: 5/25
Epoch 33/300
Validation Loss: 7.1139e-04
No improvement in validation loss. Patience: 6/25
Epoch 34/300
Validation Loss: 2.8303e-04
No improvement in validation loss. Patience: 7/25
Epoch 35/300
Validation Loss: 2.0136e-04
New best validation loss: 2.0136e-04
Epoch 36/300
Validation Loss: 3.4689e-04
No improvement in validation loss. Patience: 1/25
Epoch 37/300
Validation Loss: 3.5435e-04
No improvement in validation loss. Patience: 2/25
Epoch 38/300
Validation Loss: 2.4992e-04
No improvement in validation loss. Patience: 3/25
Epoch 39/300
Validation Loss: 2.4040e-04
No improvement in validation loss. Patience: 4/25
Epoch 40/300
Validation Loss: 4.1757e-04
No improvement in validation loss. Patience: 5/25
Epoch 41/300
Validation Loss: 1.9230e-04
New best validation loss: 1.9230e-04
Epoch 42/300
Validation Loss: 1.6038e-04
New best validation loss: 1.6038e-04
Epoch 43/300
Validation Loss: 6.0857e-04
No improvement in validation loss. Patience: 1/25
Epoch 44/300
Validation Loss: 1.0825e-03
No improvement in validation loss. Patience: 2/25
Epoch 45/300
Validation Loss: 1.7836e-04
No improvement in validation loss. Patience: 3/25
Epoch 46/300
Validation Loss: 4.6128e-04
No improvement in validation loss. Patience: 4/25
Epoch 47/300
Validation Loss: 1.5883e-04
New best validation loss: 1.5883e-04
Epoch 48/300
Validation Loss: 1.4260e-03
No improvement in validation loss. Patience: 1/25
Epoch 49/300
Validation Loss: 1.8444e-04
No improvement in validation loss. Patience: 2/25
Epoch 50/300
Validation Loss: 1.6249e-04
No improvement in validation loss. Patience: 3/25
Epoch 51/300
Validation Loss: 3.6495e-03
No improvement in validation loss. Patience: 4/25
Epoch 52/300
Validation Loss: 2.4748e-04
No improvement in validation loss. Patience: 5/25
Epoch 53/300
Validation Loss: 2.1485e-04
No improvement in validation loss. Patience: 6/25
Epoch 54/300
Validation Loss: 1.5375e-04
New best validation loss: 1.5375e-04
Epoch 55/300
Validation Loss: 1.6089e-04
No improvement in validation loss. Patience: 1/25
Epoch 56/300
Validation Loss: 2.0997e-04
No improvement in validation loss. Patience: 2/25
Epoch 57/300
Validation Loss: 2.3316e-03
No improvement in validation loss. Patience: 3/25
Epoch 58/300
Validation Loss: 2.1183e-04
No improvement in validation loss. Patience: 4/25
Epoch 59/300
Validation Loss: 1.3068e-04
New best validation loss: 1.3068e-04
Epoch 60/300
Validation Loss: 2.5611e-04
No improvement in validation loss. Patience: 1/25
Epoch 61/300
Validation Loss: 1.2255e-04
New best validation loss: 1.2255e-04
Epoch 62/300
Validation Loss: 1.8824e-04
No improvement in validation loss. Patience: 1/25
Epoch 63/300
Validation Loss: 1.2519e-04
No improvement in validation loss. Patience: 2/25
Epoch 64/300
Validation Loss: 1.3753e-04
No improvement in validation loss. Patience: 3/25
Epoch 65/300
Validation Loss: 1.3663e-04
No improvement in validation loss. Patience: 4/25
Epoch 66/300
Validation Loss: 1.5362e-04
No improvement in validation loss. Patience: 5/25
Epoch 67/300
Validation Loss: 3.5264e-04
No improvement in validation loss. Patience: 6/25
Epoch 68/300
Validation Loss: 1.7422e-04
No improvement in validation loss. Patience: 7/25
Epoch 69/300
Validation Loss: 1.4141e-04
No improvement in validation loss. Patience: 8/25
Epoch 70/300
Validation Loss: 2.2709e-04
No improvement in validation loss. Patience: 9/25
Epoch 71/300
Validation Loss: 1.3896e-04
No improvement in validation loss. Patience: 10/25
Epoch 72/300
Validation Loss: 1.5153e-04
No improvement in validation loss. Patience: 11/25
Epoch 73/300
Validation Loss: 1.8941e-04
No improvement in validation loss. Patience: 12/25
Epoch 74/300
Validation Loss: 1.3725e-04
No improvement in validation loss. Patience: 13/25
Epoch 75/300
Validation Loss: 1.2636e-04
No improvement in validation loss. Patience: 14/25
Epoch 76/300
Validation Loss: 1.4448e-04
No improvement in validation loss. Patience: 15/25
Epoch 77/300
Validation Loss: 1.1410e-04
New best validation loss: 1.1410e-04
Epoch 78/300
Validation Loss: 1.9598e-04
No improvement in validation loss. Patience: 1/25
Epoch 79/300
Validation Loss: 1.9854e-04
No improvement in validation loss. Patience: 2/25
Epoch 80/300
Validation Loss: 1.1139e-04
New best validation loss: 1.1139e-04
Epoch 81/300
Validation Loss: 1.1045e-04
New best validation loss: 1.1045e-04
Epoch 82/300
Validation Loss: 1.1760e-04
No improvement in validation loss. Patience: 1/25
Epoch 83/300
Validation Loss: 1.3697e-04
No improvement in validation loss. Patience: 2/25
Epoch 84/300
Validation Loss: 1.0791e-04
New best validation loss: 1.0791e-04
Epoch 85/300
Validation Loss: 1.3190e-04
No improvement in validation loss. Patience: 1/25
Epoch 86/300
Validation Loss: 1.7392e-04
No improvement in validation loss. Patience: 2/25
Epoch 87/300
Validation Loss: 1.3590e-04
No improvement in validation loss. Patience: 3/25
Epoch 88/300
Validation Loss: 1.0492e-04
New best validation loss: 1.0492e-04
Epoch 89/300
Validation Loss: 9.7012e-04
No improvement in validation loss. Patience: 1/25
Epoch 90/300
Validation Loss: 1.2493e-03
No improvement in validation loss. Patience: 2/25
Epoch 91/300
Validation Loss: 1.2210e-04
No improvement in validation loss. Patience: 3/25
Epoch 92/300
Validation Loss: 9.8928e-05
New best validation loss: 9.8928e-05
Epoch 93/300
Validation Loss: 9.2158e-05
New best validation loss: 9.2158e-05
Epoch 94/300
Validation Loss: 8.5120e-05
New best validation loss: 8.5120e-05
Epoch 95/300
Validation Loss: 9.9308e-05
No improvement in validation loss. Patience: 1/25
Epoch 96/300
Validation Loss: 1.1759e-04
No improvement in validation loss. Patience: 2/25
Epoch 97/300
Validation Loss: 1.1122e-04
No improvement in validation loss. Patience: 3/25
Epoch 98/300
Validation Loss: 1.1912e-04
No improvement in validation loss. Patience: 4/25
Epoch 99/300
Validation Loss: 1.5251e-04
No improvement in validation loss. Patience: 5/25
Epoch 100/300
Validation Loss: 1.1040e-04
No improvement in validation loss. Patience: 6/25
Epoch 101/300
Validation Loss: 1.8114e-04
No improvement in validation loss. Patience: 7/25
Epoch 102/300
Validation Loss: 8.6865e-05
No improvement in validation loss. Patience: 8/25
Epoch 103/300
Validation Loss: 8.2497e-05
New best validation loss: 8.2497e-05
Epoch 104/300
Validation Loss: 8.6620e-05
No improvement in validation loss. Patience: 1/25
Epoch 105/300
Validation Loss: 9.7854e-05
No improvement in validation loss. Patience: 2/25
Epoch 106/300
Validation Loss: 1.3397e-04
No improvement in validation loss. Patience: 3/25
Epoch 107/300
Validation Loss: 1.5754e-04
No improvement in validation loss. Patience: 4/25
Epoch 108/300
Validation Loss: 1.0564e-04
No improvement in validation loss. Patience: 5/25
Epoch 109/300
Validation Loss: 1.2072e-03
No improvement in validation loss. Patience: 6/25
Epoch 110/300
Validation Loss: 1.0940e-04
No improvement in validation loss. Patience: 7/25
Epoch 111/300
Validation Loss: 1.0606e-04
No improvement in validation loss. Patience: 8/25
Epoch 112/300
Validation Loss: 1.0482e-04
No improvement in validation loss. Patience: 9/25
Epoch 113/300
Validation Loss: 1.1708e-04
No improvement in validation loss. Patience: 10/25
Epoch 114/300
Validation Loss: 1.0887e-04
No improvement in validation loss. Patience: 11/25
Epoch 115/300
Validation Loss: 8.7374e-05
No improvement in validation loss. Patience: 12/25
Epoch 116/300
Validation Loss: 9.1157e-05
No improvement in validation loss. Patience: 13/25
Epoch 117/300
Validation Loss: 1.5422e-04
No improvement in validation loss. Patience: 14/25
Epoch 118/300
Validation Loss: 1.1868e-04
No improvement in validation loss. Patience: 15/25
Epoch 119/300
Validation Loss: 9.3437e-05
No improvement in validation loss. Patience: 16/25
Epoch 120/300
Validation Loss: 1.2108e-04
No improvement in validation loss. Patience: 17/25
Epoch 121/300
Validation Loss: 9.5764e-05
No improvement in validation loss. Patience: 18/25
Epoch 122/300
Validation Loss: 8.6099e-05
No improvement in validation loss. Patience: 19/25
Epoch 123/300
Validation Loss: 9.9355e-05
No improvement in validation loss. Patience: 20/25
Epoch 124/300
Validation Loss: 1.1406e-04
No improvement in validation loss. Patience: 21/25
Epoch 125/300
Validation Loss: 1.2207e-04
No improvement in validation loss. Patience: 22/25
Epoch 126/300
Validation Loss: 9.4544e-05
No improvement in validation loss. Patience: 23/25
Epoch 127/300
Validation Loss: 9.6007e-05
No improvement in validation loss. Patience: 24/25
Epoch 128/300
Validation Loss: 7.1833e-05
New best validation loss: 7.1833e-05
Epoch 129/300
Validation Loss: 8.7225e-05
No improvement in validation loss. Patience: 1/25
Epoch 130/300
Validation Loss: 7.8760e-05
No improvement in validation loss. Patience: 2/25
Epoch 131/300
Validation Loss: 5.7800e-04
No improvement in validation loss. Patience: 3/25
Epoch 132/300
Validation Loss: 9.4728e-04
No improvement in validation loss. Patience: 4/25
Epoch 133/300
Validation Loss: 9.6832e-05
No improvement in validation loss. Patience: 5/25
Epoch 134/300
Validation Loss: 9.3648e-05
No improvement in validation loss. Patience: 6/25
Epoch 135/300
Validation Loss: 9.9554e-05
No improvement in validation loss. Patience: 7/25
Epoch 136/300
Validation Loss: 2.0186e-04
No improvement in validation loss. Patience: 8/25
Epoch 137/300
Validation Loss: 1.0727e-04
No improvement in validation loss. Patience: 9/25
Epoch 138/300
Validation Loss: 9.3068e-05
No improvement in validation loss. Patience: 10/25
Epoch 139/300
Validation Loss: 1.1435e-04
No improvement in validation loss. Patience: 11/25
Epoch 140/300
Validation Loss: 7.5061e-05
No improvement in validation loss. Patience: 12/25
Epoch 141/300
Validation Loss: 1.0012e-04
No improvement in validation loss. Patience: 13/25
Epoch 142/300
Validation Loss: 1.0516e-04
No improvement in validation loss. Patience: 14/25
Epoch 143/300
Validation Loss: 8.7851e-05
No improvement in validation loss. Patience: 15/25
Epoch 144/300
Validation Loss: 8.7865e-05
No improvement in validation loss. Patience: 16/25
Epoch 145/300
Validation Loss: 1.1220e-04
No improvement in validation loss. Patience: 17/25
Epoch 146/300
Validation Loss: 7.5442e-05
No improvement in validation loss. Patience: 18/25
Epoch 147/300
Validation Loss: 8.1186e-05
No improvement in validation loss. Patience: 19/25
Epoch 148/300
Validation Loss: 7.6565e-05
No improvement in validation loss. Patience: 20/25
Epoch 149/300
Validation Loss: 1.0663e-04
No improvement in validation loss. Patience: 21/25
Epoch 150/300
Validation Loss: 8.4646e-05
No improvement in validation loss. Patience: 22/25
Epoch 151/300
Validation Loss: 8.8240e-05
No improvement in validation loss. Patience: 23/25
Epoch 152/300
Validation Loss: 8.5400e-05
No improvement in validation loss. Patience: 24/25
Epoch 153/300
Validation Loss: 1.3265e-04
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 153.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/fork-like_superbit_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: True
  type: fork-like
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test galaxy images: (5000, 53, 53)
Shape of test PSF images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: fork-like_superbit_low-noise153
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m1.705732e-01[0m
Average Bias from ShearNet: [1m[93m-2.814149e-01[0m
Time taken: [1m[96m6.88 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 9.116790e-02, Bias = -2.505676e-03
             g2: MSE = 1.087348e-01, Bias = -2.710937e-01
  g1g2_combined: MSE = 9.995134e-02, Bias = -1.367997e-01
          sigma: MSE = 2.350000e-02, Bias = -1.624642e-01
           flux: MSE = 4.588902e-01, Bias = -6.895961e-01

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m3.028608e-02[0m
Average Bias from NGmix: [1m[93m-5.400804e-02[0m
Time taken: [1m[96m108.01 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.280790e-03, Bias = +1.534900e-03
             g2: MSE = 1.376378e-03, Bias = +6.232916e-05
  g1g2_combined: MSE = 1.328584e-03, Bias = +7.986145e-04
          sigma: MSE = 2.215738e-02, Bias = +1.457608e-01
           flux: MSE = 9.632977e-02, Bias = -3.633901e-01


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 1.360201e-02
Average Bias from MOM: 7.324185e-03
Time taken: 23.25 seconds

=== Per-Label Metrics ===
             g1: MSE = 1.361859e-02, Bias = +1.573085e-03
             g2: MSE = 1.358542e-02, Bias = +1.307529e-02
  g1g2_combined: MSE = 1.360201e-02, Bias = +7.324185e-03


Generating plots...
Plotting residuals...
Plotting galaxy samples...
Plotting psf samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/old_cnn/ideal_psf/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: old-cnn_ideal_high-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/old-cnn_ideal_high-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/old-cnn_ideal_high-noise/architecture.py
Epoch 1/300
Validation Loss: 1.4278e-02
New best validation loss: 1.4278e-02
Epoch 2/300
Validation Loss: 1.2130e-02
New best validation loss: 1.2130e-02
Epoch 3/300
Validation Loss: 1.2257e-02
No improvement in validation loss. Patience: 1/25
Epoch 4/300
Validation Loss: 1.2306e-02
No improvement in validation loss. Patience: 2/25
Epoch 5/300
Validation Loss: 1.1089e-02
New best validation loss: 1.1089e-02
Epoch 6/300
Validation Loss: 1.1200e-02
No improvement in validation loss. Patience: 1/25
Epoch 7/300
Validation Loss: 1.1262e-02
No improvement in validation loss. Patience: 2/25
Epoch 8/300
Validation Loss: 1.0607e-02
New best validation loss: 1.0607e-02
Epoch 9/300
Validation Loss: 1.0359e-02
New best validation loss: 1.0359e-02
Epoch 10/300
Validation Loss: 1.0892e-02
No improvement in validation loss. Patience: 1/25
Epoch 11/300
Validation Loss: 1.1662e-02
No improvement in validation loss. Patience: 2/25
Epoch 12/300
Validation Loss: 1.0186e-02
New best validation loss: 1.0186e-02
Epoch 13/300
Validation Loss: 1.0855e-02
No improvement in validation loss. Patience: 1/25
Epoch 14/300
Validation Loss: 1.0313e-02
No improvement in validation loss. Patience: 2/25
Epoch 15/300
Validation Loss: 9.9684e-03
New best validation loss: 9.9684e-03
Epoch 16/300
Validation Loss: 9.8779e-03
New best validation loss: 9.8779e-03
Epoch 17/300
Validation Loss: 9.9514e-03
No improvement in validation loss. Patience: 1/25
Epoch 18/300
Validation Loss: 9.9454e-03
No improvement in validation loss. Patience: 2/25
Epoch 19/300
Validation Loss: 1.1982e-02
No improvement in validation loss. Patience: 3/25
Epoch 20/300
Validation Loss: 1.0347e-02
No improvement in validation loss. Patience: 4/25
Epoch 21/300
Validation Loss: 9.8043e-03
New best validation loss: 9.8043e-03
Epoch 22/300
Validation Loss: 1.0046e-02
No improvement in validation loss. Patience: 1/25
Epoch 23/300
Validation Loss: 1.0532e-02
No improvement in validation loss. Patience: 2/25
Epoch 24/300
Validation Loss: 1.0368e-02
No improvement in validation loss. Patience: 3/25
Epoch 25/300
Validation Loss: 1.0273e-02
No improvement in validation loss. Patience: 4/25
Epoch 26/300
Validation Loss: 1.0802e-02
No improvement in validation loss. Patience: 5/25
Epoch 27/300
Validation Loss: 1.0294e-02
No improvement in validation loss. Patience: 6/25
Epoch 28/300
Validation Loss: 1.0138e-02
No improvement in validation loss. Patience: 7/25
Epoch 29/300
Validation Loss: 1.0329e-02
No improvement in validation loss. Patience: 8/25
Epoch 30/300
Validation Loss: 1.1215e-02
No improvement in validation loss. Patience: 9/25
Epoch 31/300
Validation Loss: 1.0740e-02
No improvement in validation loss. Patience: 10/25
Epoch 32/300
Validation Loss: 1.0207e-02
No improvement in validation loss. Patience: 11/25
Epoch 33/300
Validation Loss: 1.0578e-02
No improvement in validation loss. Patience: 12/25
Epoch 34/300
Validation Loss: 1.0321e-02
No improvement in validation loss. Patience: 13/25
Epoch 35/300
Validation Loss: 1.1433e-02
No improvement in validation loss. Patience: 14/25
Epoch 36/300
Validation Loss: 1.0488e-02
No improvement in validation loss. Patience: 15/25
Epoch 37/300
Validation Loss: 1.0653e-02
No improvement in validation loss. Patience: 16/25
Epoch 38/300
Validation Loss: 1.0991e-02
No improvement in validation loss. Patience: 17/25
Epoch 39/300
Validation Loss: 1.0672e-02
No improvement in validation loss. Patience: 18/25
Epoch 40/300
Validation Loss: 1.0839e-02
No improvement in validation loss. Patience: 19/25
Epoch 41/300
Validation Loss: 1.1392e-02
No improvement in validation loss. Patience: 20/25
Epoch 42/300
Validation Loss: 1.1019e-02
No improvement in validation loss. Patience: 21/25
Epoch 43/300
Validation Loss: 1.1231e-02
No improvement in validation loss. Patience: 22/25
Epoch 44/300
Validation Loss: 1.1475e-02
No improvement in validation loss. Patience: 23/25
Epoch 45/300
Validation Loss: 1.1644e-02
No improvement in validation loss. Patience: 24/25
Epoch 46/300
Validation Loss: 1.1431e-02
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 46.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/old-cnn_ideal_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: old-cnn_ideal_high-noise46
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m1.089373e-02[0m
Average Bias from ShearNet: [1m[93m6.955960e-03[0m
Time taken: [1m[96m2.26 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 4.507086e-03, Bias = -3.089763e-03
             g2: MSE = 4.665372e-03, Bias = +1.059227e-02
  g1g2_combined: MSE = 4.586227e-03, Bias = +3.751251e-03
          sigma: MSE = 2.827178e-03, Bias = -1.261564e-03
           flux: MSE = 3.157530e-02, Bias = +2.158289e-02

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96
[NaN Filter] Removed 43 rows with NaNs in predictions.

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m2.283359e-02[0m
Average Bias from NGmix: [1m[93m1.988427e-02[0m
Time taken: [1m[96m107.47 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 7.240312e-03, Bias = +2.141108e-02
             g2: MSE = 6.193483e-03, Bias = +7.856947e-04
  g1g2_combined: MSE = 6.716898e-03, Bias = +1.109839e-02
          sigma: MSE = 1.666478e-02, Bias = +6.889246e-02
           flux: MSE = 6.123579e-02, Bias = -1.155217e-02


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 1.796725e+02
Average Bias from MOM: 6.632570e-02
Time taken: 23.27 seconds

=== Per-Label Metrics ===
             g1: MSE = 5.356354e+01, Bias = -1.708099e-01
             g2: MSE = 3.057815e+02, Bias = +3.034613e-01
  g1g2_combined: MSE = 1.796725e+02, Bias = +6.632570e-02

[NaN Filter] Removed 43 rows with NaNs in predictions.

Generating plots...
Plotting residuals...
Plotting samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/old_cnn/ideal_psf/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: old-cnn_ideal_low-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/old-cnn_ideal_low-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/old-cnn_ideal_low-noise/architecture.py
Epoch 1/300
Validation Loss: 2.5506e-03
New best validation loss: 2.5506e-03
Epoch 2/300
Validation Loss: 1.0119e-03
New best validation loss: 1.0119e-03
Epoch 3/300
Validation Loss: 5.6685e-04
New best validation loss: 5.6685e-04
Epoch 4/300
Validation Loss: 5.1978e-04
New best validation loss: 5.1978e-04
Epoch 5/300
Validation Loss: 6.7776e-04
No improvement in validation loss. Patience: 1/25
Epoch 6/300
Validation Loss: 3.2898e-04
New best validation loss: 3.2898e-04
Epoch 7/300
Validation Loss: 5.0371e-04
No improvement in validation loss. Patience: 1/25
Epoch 8/300
Validation Loss: 2.9981e-04
New best validation loss: 2.9981e-04
Epoch 9/300
Validation Loss: 3.4840e-04
No improvement in validation loss. Patience: 1/25
Epoch 10/300
Validation Loss: 2.8128e-04
New best validation loss: 2.8128e-04
Epoch 11/300
Validation Loss: 2.2461e-04
New best validation loss: 2.2461e-04
Epoch 12/300
Validation Loss: 2.2060e-04
New best validation loss: 2.2060e-04
Epoch 13/300
Validation Loss: 1.7139e-04
New best validation loss: 1.7139e-04
Epoch 14/300
Validation Loss: 2.2813e-04
No improvement in validation loss. Patience: 1/25
Epoch 15/300
Validation Loss: 2.5489e-04
No improvement in validation loss. Patience: 2/25
Epoch 16/300
Validation Loss: 2.7790e-04
No improvement in validation loss. Patience: 3/25
Epoch 17/300
Validation Loss: 1.4636e-04
New best validation loss: 1.4636e-04
Epoch 18/300
Validation Loss: 1.5494e-04
No improvement in validation loss. Patience: 1/25
Epoch 19/300
Validation Loss: 1.5129e-04
No improvement in validation loss. Patience: 2/25
Epoch 20/300
Validation Loss: 1.4236e-04
New best validation loss: 1.4236e-04
Epoch 21/300
Validation Loss: 1.8338e-04
No improvement in validation loss. Patience: 1/25
Epoch 22/300
Validation Loss: 1.3462e-04
New best validation loss: 1.3462e-04
Epoch 23/300
Validation Loss: 1.1406e-04
New best validation loss: 1.1406e-04
Epoch 24/300
Validation Loss: 1.6952e-04
No improvement in validation loss. Patience: 1/25
Epoch 25/300
Validation Loss: 9.6132e-05
New best validation loss: 9.6132e-05
Epoch 26/300
Validation Loss: 1.4278e-04
No improvement in validation loss. Patience: 1/25
Epoch 27/300
Validation Loss: 9.6421e-05
No improvement in validation loss. Patience: 2/25
Epoch 28/300
Validation Loss: 1.2301e-04
No improvement in validation loss. Patience: 3/25
Epoch 29/300
Validation Loss: 1.4756e-04
No improvement in validation loss. Patience: 4/25
Epoch 30/300
Validation Loss: 1.4198e-04
No improvement in validation loss. Patience: 5/25
Epoch 31/300
Validation Loss: 1.1651e-04
No improvement in validation loss. Patience: 6/25
Epoch 32/300
Validation Loss: 1.4833e-04
No improvement in validation loss. Patience: 7/25
Epoch 33/300
Validation Loss: 7.3238e-05
New best validation loss: 7.3238e-05
Epoch 34/300
Validation Loss: 1.7389e-04
No improvement in validation loss. Patience: 1/25
Epoch 35/300
Validation Loss: 8.3765e-05
No improvement in validation loss. Patience: 2/25
Epoch 36/300
Validation Loss: 7.8950e-05
No improvement in validation loss. Patience: 3/25
Epoch 37/300
Validation Loss: 1.0771e-04
No improvement in validation loss. Patience: 4/25
Epoch 38/300
Validation Loss: 8.2241e-05
No improvement in validation loss. Patience: 5/25
Epoch 39/300
Validation Loss: 1.1707e-04
No improvement in validation loss. Patience: 6/25
Epoch 40/300
Validation Loss: 9.2818e-05
No improvement in validation loss. Patience: 7/25
Epoch 41/300
Validation Loss: 1.0102e-04
No improvement in validation loss. Patience: 8/25
Epoch 42/300
Validation Loss: 7.8317e-05
No improvement in validation loss. Patience: 9/25
Epoch 43/300
Validation Loss: 1.0696e-04
No improvement in validation loss. Patience: 10/25
Epoch 44/300
Validation Loss: 2.0932e-04
No improvement in validation loss. Patience: 11/25
Epoch 45/300
Validation Loss: 7.7636e-05
No improvement in validation loss. Patience: 12/25
Epoch 46/300
Validation Loss: 1.0422e-04
No improvement in validation loss. Patience: 13/25
Epoch 47/300
Validation Loss: 5.8702e-05
New best validation loss: 5.8702e-05
Epoch 48/300
Validation Loss: 9.2615e-05
No improvement in validation loss. Patience: 1/25
Epoch 49/300
Validation Loss: 8.2341e-05
No improvement in validation loss. Patience: 2/25
Epoch 50/300
Validation Loss: 7.1274e-05
No improvement in validation loss. Patience: 3/25
Epoch 51/300
Validation Loss: 7.1452e-05
No improvement in validation loss. Patience: 4/25
Epoch 52/300
Validation Loss: 6.8791e-05
No improvement in validation loss. Patience: 5/25
Epoch 53/300
Validation Loss: 7.0080e-05
No improvement in validation loss. Patience: 6/25
Epoch 54/300
Validation Loss: 5.6512e-05
New best validation loss: 5.6512e-05
Epoch 55/300
Validation Loss: 1.0752e-04
No improvement in validation loss. Patience: 1/25
Epoch 56/300
Validation Loss: 3.6616e-04
No improvement in validation loss. Patience: 2/25
Epoch 57/300
Validation Loss: 1.2489e-04
No improvement in validation loss. Patience: 3/25
Epoch 58/300
Validation Loss: 6.0564e-05
No improvement in validation loss. Patience: 4/25
Epoch 59/300
Validation Loss: 6.6515e-05
No improvement in validation loss. Patience: 5/25
Epoch 60/300
Validation Loss: 7.0708e-05
No improvement in validation loss. Patience: 6/25
Epoch 61/300
Validation Loss: 6.6449e-05
No improvement in validation loss. Patience: 7/25
Epoch 62/300
Validation Loss: 7.5799e-05
No improvement in validation loss. Patience: 8/25
Epoch 63/300
Validation Loss: 7.6034e-05
No improvement in validation loss. Patience: 9/25
Epoch 64/300
Validation Loss: 1.0560e-04
No improvement in validation loss. Patience: 10/25
Epoch 65/300
Validation Loss: 8.3912e-05
No improvement in validation loss. Patience: 11/25
Epoch 66/300
Validation Loss: 6.7883e-05
No improvement in validation loss. Patience: 12/25
Epoch 67/300
Validation Loss: 9.6693e-05
No improvement in validation loss. Patience: 13/25
Epoch 68/300
Validation Loss: 7.9033e-05
No improvement in validation loss. Patience: 14/25
Epoch 69/300
Validation Loss: 8.7580e-05
No improvement in validation loss. Patience: 15/25
Epoch 70/300
Validation Loss: 8.3529e-05
No improvement in validation loss. Patience: 16/25
Epoch 71/300
Validation Loss: 9.5036e-05
No improvement in validation loss. Patience: 17/25
Epoch 72/300
Validation Loss: 1.9404e-04
No improvement in validation loss. Patience: 18/25
Epoch 73/300
Validation Loss: 8.6731e-05
No improvement in validation loss. Patience: 19/25
Epoch 74/300
Validation Loss: 9.1727e-05
No improvement in validation loss. Patience: 20/25
Epoch 75/300
Validation Loss: 6.9405e-05
No improvement in validation loss. Patience: 21/25
Epoch 76/300
Validation Loss: 9.8004e-05
No improvement in validation loss. Patience: 22/25
Epoch 77/300
Validation Loss: 7.3733e-05
No improvement in validation loss. Patience: 23/25
Epoch 78/300
Validation Loss: 7.1698e-05
No improvement in validation loss. Patience: 24/25
Epoch 79/300
Validation Loss: 8.1105e-05
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 79.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/old-cnn_ideal_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: old-cnn_ideal_low-noise79
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m5.424734e-05[0m
Average Bias from ShearNet: [1m[93m1.834489e-03[0m
Time taken: [1m[96m2.25 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.696679e-05, Bias = -1.335033e-03
             g2: MSE = 2.208262e-05, Bias = +1.303995e-03
  g1g2_combined: MSE = 1.952470e-05, Bias = -1.551912e-05
          sigma: MSE = 2.522207e-05, Bias = +7.608840e-04
           flux: MSE = 1.527179e-04, Bias = +6.608113e-03

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m3.825412e-03[0m
Average Bias from NGmix: [1m[93m2.028641e-02[0m
Time taken: [1m[96m106.90 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.662816e-04, Bias = -3.767501e-04
             g2: MSE = 1.123920e-04, Bias = -1.856154e-04
  g1g2_combined: MSE = 1.393368e-04, Bias = -2.811827e-04
          sigma: MSE = 1.480912e-02, Bias = +8.235852e-02
           flux: MSE = 2.138579e-04, Bias = -6.505359e-04


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 4.643240e-03
Average Bias from MOM: -1.587618e-03
Time taken: 23.18 seconds

=== Per-Label Metrics ===
             g1: MSE = 4.552957e-03, Bias = +9.907140e-04
             g2: MSE = 4.733522e-03, Bias = -4.165950e-03
  g1g2_combined: MSE = 4.643240e-03, Bias = -1.587618e-03


Generating plots...
Plotting residuals...
Plotting samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/old_cnn/superbit_psf/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: old-cnn_superbit_high-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/old-cnn_superbit_high-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/old-cnn_superbit_high-noise/architecture.py
Epoch 1/300
Validation Loss: 1.9534e-02
New best validation loss: 1.9534e-02
Epoch 2/300
Validation Loss: 1.6460e-02
New best validation loss: 1.6460e-02
Epoch 3/300
Validation Loss: 1.5961e-02
New best validation loss: 1.5961e-02
Epoch 4/300
Validation Loss: 1.5278e-02
New best validation loss: 1.5278e-02
Epoch 5/300
Validation Loss: 1.5565e-02
No improvement in validation loss. Patience: 1/25
Epoch 6/300
Validation Loss: 1.4488e-02
New best validation loss: 1.4488e-02
Epoch 7/300
Validation Loss: 1.4318e-02
New best validation loss: 1.4318e-02
Epoch 8/300
Validation Loss: 1.4055e-02
New best validation loss: 1.4055e-02
Epoch 9/300
Validation Loss: 1.3877e-02
New best validation loss: 1.3877e-02
Epoch 10/300
Validation Loss: 1.3998e-02
No improvement in validation loss. Patience: 1/25
Epoch 11/300
Validation Loss: 1.6110e-02
No improvement in validation loss. Patience: 2/25
Epoch 12/300
Validation Loss: 1.3602e-02
New best validation loss: 1.3602e-02
Epoch 13/300
Validation Loss: 1.3616e-02
No improvement in validation loss. Patience: 1/25
Epoch 14/300
Validation Loss: 1.3649e-02
No improvement in validation loss. Patience: 2/25
Epoch 15/300
Validation Loss: 1.3593e-02
New best validation loss: 1.3593e-02
Epoch 16/300
Validation Loss: 1.3453e-02
New best validation loss: 1.3453e-02
Epoch 17/300
Validation Loss: 1.3480e-02
No improvement in validation loss. Patience: 1/25
Epoch 18/300
Validation Loss: 1.3290e-02
New best validation loss: 1.3290e-02
Epoch 19/300
Validation Loss: 1.4583e-02
No improvement in validation loss. Patience: 1/25
Epoch 20/300
Validation Loss: 1.4185e-02
No improvement in validation loss. Patience: 2/25
Epoch 21/300
Validation Loss: 1.3900e-02
No improvement in validation loss. Patience: 3/25
Epoch 22/300
Validation Loss: 1.3965e-02
No improvement in validation loss. Patience: 4/25
Epoch 23/300
Validation Loss: 1.3493e-02
No improvement in validation loss. Patience: 5/25
Epoch 24/300
Validation Loss: 1.3492e-02
No improvement in validation loss. Patience: 6/25
Epoch 25/300
Validation Loss: 1.3696e-02
No improvement in validation loss. Patience: 7/25
Epoch 26/300
Validation Loss: 1.4130e-02
No improvement in validation loss. Patience: 8/25
Epoch 27/300
Validation Loss: 1.3777e-02
No improvement in validation loss. Patience: 9/25
Epoch 28/300
Validation Loss: 1.3899e-02
No improvement in validation loss. Patience: 10/25
Epoch 29/300
Validation Loss: 1.4181e-02
No improvement in validation loss. Patience: 11/25
Epoch 30/300
Validation Loss: 1.4114e-02
No improvement in validation loss. Patience: 12/25
Epoch 31/300
Validation Loss: 1.4675e-02
No improvement in validation loss. Patience: 13/25
Epoch 32/300
Validation Loss: 1.4322e-02
No improvement in validation loss. Patience: 14/25
Epoch 33/300
Validation Loss: 1.4187e-02
No improvement in validation loss. Patience: 15/25
Epoch 34/300
Validation Loss: 1.4282e-02
No improvement in validation loss. Patience: 16/25
Epoch 35/300
Validation Loss: 1.6387e-02
No improvement in validation loss. Patience: 17/25
Epoch 36/300
Validation Loss: 1.4546e-02
No improvement in validation loss. Patience: 18/25
Epoch 37/300
Validation Loss: 1.4716e-02
No improvement in validation loss. Patience: 19/25
Epoch 38/300
Validation Loss: 1.4954e-02
No improvement in validation loss. Patience: 20/25
Epoch 39/300
Validation Loss: 1.4741e-02
No improvement in validation loss. Patience: 21/25
Epoch 40/300
Validation Loss: 1.5090e-02
No improvement in validation loss. Patience: 22/25
Epoch 41/300
Validation Loss: 1.5138e-02
No improvement in validation loss. Patience: 23/25
Epoch 42/300
Validation Loss: 1.5150e-02
No improvement in validation loss. Patience: 24/25
Epoch 43/300
Validation Loss: 1.5566e-02
No improvement in validation loss. Patience: 25/25
Early stopping triggered.
Checkpoint saved at step 43.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/old-cnn_superbit_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: old-cnn_superbit_high-noise43
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m1.423794e-02[0m
Average Bias from ShearNet: [1m[93m4.844484e-03[0m
Time taken: [1m[96m2.10 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 7.966834e-03, Bias = +2.408282e-03
             g2: MSE = 7.789109e-03, Bias = -1.604007e-02
  g1g2_combined: MSE = 7.877972e-03, Bias = -6.815895e-03
          sigma: MSE = 4.165863e-03, Bias = +2.051254e-03
           flux: MSE = 3.702996e-02, Bias = +3.095847e-02

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96
[NaN Filter] Removed 37 rows with NaNs in predictions.

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m5.429723e-02[0m
Average Bias from NGmix: [1m[93m-5.235949e-02[0m
Time taken: [1m[96m107.94 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.039961e-02, Bias = +2.506346e-02
             g2: MSE = 9.343549e-03, Bias = -1.153685e-03
  g1g2_combined: MSE = 9.871579e-03, Bias = +1.195489e-02
          sigma: MSE = 2.512277e-02, Bias = +1.191550e-01
           flux: MSE = 1.723230e-01, Bias = -3.525028e-01


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 4.308994e+01
Average Bias from MOM: -1.244838e-02
Time taken: 23.20 seconds

=== Per-Label Metrics ===
             g1: MSE = 2.110289e+01, Bias = +6.676307e-02
             g2: MSE = 6.507699e+01, Bias = -9.165983e-02
  g1g2_combined: MSE = 4.308994e+01, Bias = -1.244838e-02

[NaN Filter] Removed 37 rows with NaNs in predictions.

Generating plots...
Plotting residuals...
Plotting samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/shearnet/old_cnn/superbit_psf/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: False
  psf_shear_range: 0.05

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 128
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: old-cnn_superbit_low-noise

plotting:
  plot: True
==================================================

Running on device: cuda:0
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Shape of train images: (100000, 53, 53)
Shape of train labels: (100000, 4)

Training configuration saved to: /home/adfield/ShearNet/plots/old-cnn_superbit_low-noise/training_config.yaml
Model architecture saved to: /home/adfield/ShearNet/plots/old-cnn_superbit_low-noise/architecture.py
Epoch 1/300
Validation Loss: 4.0332e-03
New best validation loss: 4.0332e-03
Epoch 2/300
Validation Loss: 2.2795e-03
New best validation loss: 2.2795e-03
Epoch 3/300
Validation Loss: 1.8427e-03
New best validation loss: 1.8427e-03
Epoch 4/300
Validation Loss: 1.7924e-03
New best validation loss: 1.7924e-03
Epoch 5/300
Validation Loss: 1.5224e-03
New best validation loss: 1.5224e-03
Epoch 6/300
Validation Loss: 1.7848e-03
No improvement in validation loss. Patience: 1/25
Epoch 7/300
Validation Loss: 1.3516e-03
New best validation loss: 1.3516e-03
Epoch 8/300
Validation Loss: 1.4775e-03
No improvement in validation loss. Patience: 1/25
Epoch 9/300
Validation Loss: 1.1723e-03
New best validation loss: 1.1723e-03
Epoch 10/300
Validation Loss: 1.3538e-03
No improvement in validation loss. Patience: 1/25
Epoch 11/300
Validation Loss: 1.0373e-03
New best validation loss: 1.0373e-03
Epoch 12/300
Validation Loss: 1.0175e-03
New best validation loss: 1.0175e-03
Epoch 13/300
Validation Loss: 9.6560e-04
New best validation loss: 9.6560e-04
Epoch 14/300
Validation Loss: 9.0768e-04
New best validation loss: 9.0768e-04
Epoch 15/300
Validation Loss: 1.1385e-03
No improvement in validation loss. Patience: 1/25
Epoch 16/300
Validation Loss: 8.1704e-04
New best validation loss: 8.1704e-04
Epoch 17/300
Validation Loss: 7.7804e-04
New best validation loss: 7.7804e-04
Epoch 18/300
Validation Loss: 8.1490e-04
No improvement in validation loss. Patience: 1/25
Epoch 19/300
Validation Loss: 7.3787e-04
New best validation loss: 7.3787e-04
Epoch 20/300
Validation Loss: 7.5065e-04
No improvement in validation loss. Patience: 1/25
Epoch 21/300
Validation Loss: 6.7669e-04
New best validation loss: 6.7669e-04
Epoch 22/300
Validation Loss: 6.9276e-04
No improvement in validation loss. Patience: 1/25
Epoch 23/300
Validation Loss: 8.5419e-04
No improvement in validation loss. Patience: 2/25
Epoch 24/300
Validation Loss: 7.6080e-04
No improvement in validation loss. Patience: 3/25
Epoch 25/300
Validation Loss: 7.1430e-04
No improvement in validation loss. Patience: 4/25
Epoch 26/300
Validation Loss: 7.0773e-04
No improvement in validation loss. Patience: 5/25
Epoch 27/300
Validation Loss: 7.1736e-04
No improvement in validation loss. Patience: 6/25
Epoch 28/300
Validation Loss: 7.4774e-04
No improvement in validation loss. Patience: 7/25
Epoch 29/300
Validation Loss: 1.1943e-03
No improvement in validation loss. Patience: 8/25
Epoch 30/300
Validation Loss: 7.4455e-04
No improvement in validation loss. Patience: 9/25
Epoch 31/300
Validation Loss: 6.7947e-04
No improvement in validation loss. Patience: 10/25
Epoch 32/300
Validation Loss: 6.9739e-04
No improvement in validation loss. Patience: 11/25
Epoch 33/300
Validation Loss: 7.5604e-04
No improvement in validation loss. Patience: 12/25
Epoch 34/300
Validation Loss: 7.3072e-04
No improvement in validation loss. Patience: 13/25
Epoch 35/300
Validation Loss: 6.0155e-04
New best validation loss: 6.0155e-04
Epoch 36/300
Validation Loss: 5.9417e-04
New best validation loss: 5.9417e-04
Epoch 37/300
Validation Loss: 5.3672e-04
New best validation loss: 5.3672e-04
Epoch 38/300
Validation Loss: 5.7621e-04
No improvement in validation loss. Patience: 1/25
Epoch 39/300
Validation Loss: 5.5241e-04
No improvement in validation loss. Patience: 2/25
Epoch 40/300
Validation Loss: 5.2068e-04
New best validation loss: 5.2068e-04
Epoch 41/300
Validation Loss: 5.6428e-04
No improvement in validation loss. Patience: 1/25
Epoch 42/300
Validation Loss: 5.0123e-04
New best validation loss: 5.0123e-04
Epoch 43/300
Validation Loss: 6.2084e-04
No improvement in validation loss. Patience: 1/25
Epoch 44/300
Validation Loss: 5.1650e-04
No improvement in validation loss. Patience: 2/25
Epoch 45/300
Validation Loss: 6.6493e-04
No improvement in validation loss. Patience: 3/25
Epoch 46/300
Validation Loss: 1.2738e-02
No improvement in validation loss. Patience: 4/25
Epoch 47/300
Validation Loss: 9.3351e-04
No improvement in validation loss. Patience: 5/25
Epoch 48/300
Validation Loss: 8.4095e-04
No improvement in validation loss. Patience: 6/25
Epoch 49/300
Validation Loss: 6.9752e-04
No improvement in validation loss. Patience: 7/25
Epoch 50/300
Validation Loss: 5.3325e-04
No improvement in validation loss. Patience: 8/25
Epoch 51/300
Validation Loss: 7.6982e-04
No improvement in validation loss. Patience: 9/25
Epoch 52/300
Validation Loss: 5.5107e-04
No improvement in validation loss. Patience: 10/25
Epoch 53/300
Validation Loss: 4.6622e-04
New best validation loss: 4.6622e-04
Epoch 54/300
Validation Loss: 4.4919e-04
New best validation loss: 4.4919e-04
Epoch 55/300
Validation Loss: 4.9553e-04
No improvement in validation loss. Patience: 1/25
Epoch 56/300
Validation Loss: 4.2414e-04
New best validation loss: 4.2414e-04
Epoch 57/300
Validation Loss: 4.9206e-04
No improvement in validation loss. Patience: 1/25
Epoch 58/300
Validation Loss: 5.9585e-04
No improvement in validation loss. Patience: 2/25
Epoch 59/300
Validation Loss: 4.5018e-04
No improvement in validation loss. Patience: 3/25
Epoch 60/300
Validation Loss: 4.7893e-04
No improvement in validation loss. Patience: 4/25
Epoch 61/300
Validation Loss: 4.4868e-04
No improvement in validation loss. Patience: 5/25
Epoch 62/300
Validation Loss: 4.0506e-04
New best validation loss: 4.0506e-04
Epoch 63/300
Validation Loss: 3.9335e-04
New best validation loss: 3.9335e-04
Epoch 64/300
Validation Loss: 3.7762e-04
New best validation loss: 3.7762e-04
Epoch 65/300
Validation Loss: 3.7900e-04
No improvement in validation loss. Patience: 1/25
Epoch 66/300
Validation Loss: 4.1359e-04
No improvement in validation loss. Patience: 2/25
Epoch 67/300
Validation Loss: 4.2911e-04
No improvement in validation loss. Patience: 3/25
Epoch 68/300
Validation Loss: 3.5262e-04
New best validation loss: 3.5262e-04
Epoch 69/300
Validation Loss: 3.6488e-04
No improvement in validation loss. Patience: 1/25
Epoch 70/300
Validation Loss: 3.8706e-04
No improvement in validation loss. Patience: 2/25
Epoch 71/300
Validation Loss: 3.6474e-04
No improvement in validation loss. Patience: 3/25
Epoch 72/300
Validation Loss: 3.7712e-04
No improvement in validation loss. Patience: 4/25
Epoch 73/300
Validation Loss: 3.9476e-04
No improvement in validation loss. Patience: 5/25
Epoch 74/300
Validation Loss: 3.5336e-04
No improvement in validation loss. Patience: 6/25
Epoch 75/300
Validation Loss: 3.6866e-04
No improvement in validation loss. Patience: 7/25
Epoch 76/300
Validation Loss: 4.3459e-04
No improvement in validation loss. Patience: 8/25
Epoch 77/300
Validation Loss: 3.6682e-04
No improvement in validation loss. Patience: 9/25
Epoch 78/300
Validation Loss: 3.5729e-04
No improvement in validation loss. Patience: 10/25
Epoch 79/300
Validation Loss: 3.3282e-04
New best validation loss: 3.3282e-04
Epoch 80/300
Validation Loss: 3.4012e-04
No improvement in validation loss. Patience: 1/25
Epoch 81/300
Validation Loss: 3.4171e-04
No improvement in validation loss. Patience: 2/25
Epoch 82/300
Validation Loss: 3.3044e-04
New best validation loss: 3.3044e-04
Epoch 83/300
Validation Loss: 3.7601e-04
No improvement in validation loss. Patience: 1/25
Epoch 84/300
Validation Loss: 4.6913e-04
No improvement in validation loss. Patience: 2/25
Epoch 85/300
Validation Loss: 3.2817e-04
New best validation loss: 3.2817e-04
Epoch 86/300
Validation Loss: 3.4705e-04
No improvement in validation loss. Patience: 1/25
Epoch 87/300
Validation Loss: 3.3153e-04
No improvement in validation loss. Patience: 2/25
Epoch 88/300
Validation Loss: 3.2936e-04
No improvement in validation loss. Patience: 3/25
Epoch 89/300
Validation Loss: 3.8034e-04
No improvement in validation loss. Patience: 4/25
Epoch 90/300
Validation Loss: 1.0358e-03
No improvement in validation loss. Patience: 5/25
Epoch 91/300
Validation Loss: 3.5202e-04
No improvement in validation loss. Patience: 6/25
Epoch 92/300
Validation Loss: 3.6430e-04
No improvement in validation loss. Patience: 7/25
Epoch 93/300
Validation Loss: 3.0377e-04
New best validation loss: 3.0377e-04
Epoch 94/300
Validation Loss: 3.1168e-04
No improvement in validation loss. Patience: 1/25
Epoch 95/300
Validation Loss: 3.2185e-04
No improvement in validation loss. Patience: 2/25
Epoch 96/300
Validation Loss: 3.2887e-04
No improvement in validation loss. Patience: 3/25
Epoch 97/300
Validation Loss: 3.1500e-04
No improvement in validation loss. Patience: 4/25
Epoch 98/300
Validation Loss: 3.3033e-04
No improvement in validation loss. Patience: 5/25
Epoch 99/300
Validation Loss: 3.2548e-04
No improvement in validation loss. Patience: 6/25
Epoch 100/300
Validation Loss: 3.2034e-04
No improvement in validation loss. Patience: 7/25
Epoch 101/300
Validation Loss: 3.0625e-04
No improvement in validation loss. Patience: 8/25
Epoch 102/300
Validation Loss: 3.3149e-04
No improvement in validation loss. Patience: 9/25
Epoch 103/300
Validation Loss: 2.9090e-04
New best validation loss: 2.9090e-04
Epoch 104/300
Validation Loss: 3.0822e-04
No improvement in validation loss. Patience: 1/25
Epoch 105/300
Validation Loss: 3.3057e-04
No improvement in validation loss. Patience: 2/25
Epoch 106/300
Validation Loss: 3.2312e-04
No improvement in validation loss. Patience: 3/25
Epoch 107/300
Validation Loss: 3.1069e-04
No improvement in validation loss. Patience: 4/25
Epoch 108/300
Validation Loss: 2.8720e-04
New best validation loss: 2.8720e-04
Epoch 109/300
Validation Loss: 3.0008e-04
No improvement in validation loss. Patience: 1/25
Epoch 110/300
Validation Loss: 3.0661e-04
No improvement in validation loss. Patience: 2/25
Epoch 111/300
Validation Loss: 2.8733e-04
No improvement in validation loss. Patience: 3/25
Epoch 112/300
Validation Loss: 2.9281e-04
No improvement in validation loss. Patience: 4/25
Epoch 113/300
Validation Loss: 2.9610e-04
No improvement in validation loss. Patience: 5/25
Epoch 114/300
Validation Loss: 2.8267e-04
New best validation loss: 2.8267e-04
Epoch 115/300
Validation Loss: 3.3304e-04
No improvement in validation loss. Patience: 1/25
Epoch 116/300
Validation Loss: 2.9432e-04
No improvement in validation loss. Patience: 2/25
Epoch 117/300
Validation Loss: 3.0288e-04
No improvement in validation loss. Patience: 3/25
Epoch 118/300
Validation Loss: 2.9531e-04
No improvement in validation loss. Patience: 4/25
Epoch 119/300
Validation Loss: 3.2021e-04
No improvement in validation loss. Patience: 5/25
Epoch 120/300
Validation Loss: 3.0134e-04
No improvement in validation loss. Patience: 6/25
Epoch 121/300
Validation Loss: 2.9174e-04
No improvement in validation loss. Patience: 7/25
Epoch 122/300
Validation Loss: 2.6630e-04
New best validation loss: 2.6630e-04
Epoch 123/300
Validation Loss: 2.5992e-04
New best validation loss: 2.5992e-04
Epoch 124/300
Validation Loss: 2.8761e-04
No improvement in validation loss. Patience: 1/25
Epoch 125/300
Validation Loss: 3.0123e-04
No improvement in validation loss. Patience: 2/25
Epoch 126/300
Validation Loss: 2.8095e-04
No improvement in validation loss. Patience: 3/25
Epoch 127/300
Validation Loss: 2.8697e-04
No improvement in validation loss. Patience: 4/25
Epoch 128/300
Validation Loss: 2.5921e-04
New best validation loss: 2.5921e-04
Epoch 129/300
Validation Loss: 2.6341e-04
No improvement in validation loss. Patience: 1/25
Epoch 130/300
Validation Loss: 2.8198e-04
No improvement in validation loss. Patience: 2/25
Epoch 131/300
Validation Loss: 2.5447e-04
New best validation loss: 2.5447e-04
Epoch 132/300
Validation Loss: 2.9550e-04
No improvement in validation loss. Patience: 1/25
Epoch 133/300
Validation Loss: 2.8347e-04
No improvement in validation loss. Patience: 2/25
Epoch 134/300
Validation Loss: 2.8822e-04
No improvement in validation loss. Patience: 3/25
Epoch 135/300
Validation Loss: 2.6004e-04
No improvement in validation loss. Patience: 4/25
Epoch 136/300
Validation Loss: 2.6022e-04
No improvement in validation loss. Patience: 5/25
Epoch 137/300
Validation Loss: 2.8213e-04
No improvement in validation loss. Patience: 6/25
Epoch 138/300
Validation Loss: 2.6103e-04
No improvement in validation loss. Patience: 7/25
Epoch 139/300
Validation Loss: 2.6646e-04
No improvement in validation loss. Patience: 8/25
Epoch 140/300
Validation Loss: 2.5531e-04
No improvement in validation loss. Patience: 9/25
Epoch 141/300
Validation Loss: 2.7575e-04
No improvement in validation loss. Patience: 10/25
Epoch 142/300
Validation Loss: 2.5063e-04
New best validation loss: 2.5063e-04
Epoch 143/300
Validation Loss: 2.5287e-04
No improvement in validation loss. Patience: 1/25
Epoch 144/300
Validation Loss: 2.4908e-04
New best validation loss: 2.4908e-04
Epoch 145/300
Validation Loss: 2.5476e-04
No improvement in validation loss. Patience: 1/25
Epoch 146/300
Validation Loss: 2.5346e-04
No improvement in validation loss. Patience: 2/25
Epoch 147/300
Validation Loss: 2.8573e-04
No improvement in validation loss. Patience: 3/25
Epoch 148/300
Validation Loss: 2.7546e-04
No improvement in validation loss. Patience: 4/25
Epoch 149/300
Validation Loss: 2.6321e-04
No improvement in validation loss. Patience: 5/25
Epoch 150/300
Validation Loss: 2.4539e-04
New best validation loss: 2.4539e-04
Epoch 151/300
Validation Loss: 2.5194e-04
No improvement in validation loss. Patience: 1/25
Epoch 152/300
Validation Loss: 2.3715e-04
New best validation loss: 2.3715e-04
Epoch 153/300
Validation Loss: 2.4430e-04
No improvement in validation loss. Patience: 1/25
Epoch 154/300
Validation Loss: 2.4286e-04
No improvement in validation loss. Patience: 2/25
Epoch 155/300
Validation Loss: 2.4926e-04
No improvement in validation loss. Patience: 3/25
Epoch 156/300
Validation Loss: 2.5428e-04
No improvement in validation loss. Patience: 4/25
Epoch 157/300
Validation Loss: 2.3731e-04
No improvement in validation loss. Patience: 5/25
Epoch 158/300
Validation Loss: 2.6428e-04
No improvement in validation loss. Patience: 6/25
Epoch 159/300
Validation Loss: 2.5931e-04
No improvement in validation loss. Patience: 7/25
Epoch 160/300
Validation Loss: 2.5710e-04
No improvement in validation loss. Patience: 8/25
Epoch 161/300
Validation Loss: 2.4296e-04
No improvement in validation loss. Patience: 9/25
Epoch 162/300
Validation Loss: 2.3273e-04
New best validation loss: 2.3273e-04
Epoch 163/300
Validation Loss: 2.3262e-04
New best validation loss: 2.3262e-04
Epoch 164/300
Validation Loss: 2.3942e-04
No improvement in validation loss. Patience: 1/25
Epoch 165/300
Validation Loss: 2.3160e-04
New best validation loss: 2.3160e-04
Epoch 166/300
Validation Loss: 2.4649e-04
No improvement in validation loss. Patience: 1/25
Epoch 167/300
Validation Loss: 2.4051e-04
No improvement in validation loss. Patience: 2/25
Epoch 168/300
Validation Loss: 2.4864e-04
No improvement in validation loss. Patience: 3/25
Epoch 169/300
Validation Loss: 2.4494e-04
No improvement in validation loss. Patience: 4/25
Epoch 170/300
Validation Loss: 2.3678e-04
No improvement in validation loss. Patience: 5/25
Epoch 171/300
Validation Loss: 2.3475e-04
No improvement in validation loss. Patience: 6/25
Epoch 172/300
Validation Loss: 2.3795e-04
No improvement in validation loss. Patience: 7/25
Epoch 173/300
Validation Loss: 2.3759e-04
No improvement in validation loss. Patience: 8/25
Epoch 174/300
Validation Loss: 2.4535e-04
No improvement in validation loss. Patience: 9/25
Epoch 175/300
Validation Loss: 2.5332e-04
No improvement in validation loss. Patience: 10/25
Epoch 176/300
Validation Loss: 2.3093e-04
New best validation loss: 2.3093e-04
Epoch 177/300
Validation Loss: 2.4667e-04
No improvement in validation loss. Patience: 1/25
Epoch 178/300
Validation Loss: 2.2769e-04
New best validation loss: 2.2769e-04
Epoch 179/300
Validation Loss: 2.2579e-04
New best validation loss: 2.2579e-04
Epoch 180/300
Validation Loss: 2.3697e-04
No improvement in validation loss. Patience: 1/25
Epoch 181/300
Validation Loss: 2.2195e-04
New best validation loss: 2.2195e-04
Epoch 182/300
Validation Loss: 2.3037e-04
No improvement in validation loss. Patience: 1/25
Epoch 183/300
Validation Loss: 2.3383e-04
No improvement in validation loss. Patience: 2/25
Epoch 184/300
Validation Loss: 2.2744e-04
No improvement in validation loss. Patience: 3/25
Epoch 185/300
Validation Loss: 2.3155e-04
No improvement in validation loss. Patience: 4/25
Epoch 186/300
Validation Loss: 2.3023e-04
No improvement in validation loss. Patience: 5/25
Epoch 187/300
Validation Loss: 2.2283e-04
No improvement in validation loss. Patience: 6/25
Epoch 188/300
Validation Loss: 2.2603e-04
No improvement in validation loss. Patience: 7/25
Epoch 189/300
Validation Loss: 2.1770e-04
New best validation loss: 2.1770e-04
Epoch 190/300
Validation Loss: 2.2249e-04
No improvement in validation loss. Patience: 1/25
Epoch 191/300
Validation Loss: 2.2394e-04
No improvement in validation loss. Patience: 2/25
Epoch 192/300
Validation Loss: 2.1755e-04
New best validation loss: 2.1755e-04
Epoch 193/300
Validation Loss: 2.3961e-04
No improvement in validation loss. Patience: 1/25
Epoch 194/300
Validation Loss: 2.2817e-04
No improvement in validation loss. Patience: 2/25
Epoch 195/300
Validation Loss: 2.1861e-04
No improvement in validation loss. Patience: 3/25
Epoch 196/300
Validation Loss: 2.2364e-04
No improvement in validation loss. Patience: 4/25
Epoch 197/300
Validation Loss: 2.1526e-04
New best validation loss: 2.1526e-04
Epoch 198/300
Validation Loss: 2.1356e-04
New best validation loss: 2.1356e-04
Epoch 199/300
Validation Loss: 2.1930e-04
No improvement in validation loss. Patience: 1/25
Epoch 200/300
Validation Loss: 2.1517e-04
No improvement in validation loss. Patience: 2/25
Epoch 201/300
Validation Loss: 2.2383e-04
No improvement in validation loss. Patience: 3/25
Epoch 202/300
Validation Loss: 2.3183e-04
No improvement in validation loss. Patience: 4/25
Epoch 203/300
Validation Loss: 2.3987e-04
No improvement in validation loss. Patience: 5/25
Epoch 204/300
Validation Loss: 2.1653e-04
No improvement in validation loss. Patience: 6/25
Epoch 205/300
Validation Loss: 2.1962e-04
No improvement in validation loss. Patience: 7/25
Epoch 206/300
Validation Loss: 2.3720e-04
No improvement in validation loss. Patience: 8/25
Epoch 207/300
Validation Loss: 2.2967e-04
No improvement in validation loss. Patience: 9/25
Epoch 208/300
Validation Loss: 2.1443e-04
No improvement in validation loss. Patience: 10/25
Epoch 209/300
Validation Loss: 2.1746e-04
No improvement in validation loss. Patience: 11/25
Epoch 210/300
Validation Loss: 2.2458e-04
No improvement in validation loss. Patience: 12/25
Epoch 211/300
Validation Loss: 2.1110e-04
New best validation loss: 2.1110e-04
Epoch 212/300
Validation Loss: 2.2231e-04
No improvement in validation loss. Patience: 1/25
Epoch 213/300
Validation Loss: 2.1133e-04
No improvement in validation loss. Patience: 2/25
Epoch 214/300
Validation Loss: 2.1752e-04
No improvement in validation loss. Patience: 3/25
Epoch 215/300
Validation Loss: 2.1173e-04
No improvement in validation loss. Patience: 4/25
Epoch 216/300
Validation Loss: 2.0977e-04
New best validation loss: 2.0977e-04
Epoch 217/300
Validation Loss: 2.1067e-04
No improvement in validation loss. Patience: 1/25
Epoch 218/300
Validation Loss: 2.1382e-04
No improvement in validation loss. Patience: 2/25
Epoch 219/300
Validation Loss: 2.0948e-04
New best validation loss: 2.0948e-04
Epoch 220/300
Validation Loss: 2.1752e-04
No improvement in validation loss. Patience: 1/25
Epoch 221/300
Validation Loss: 2.0907e-04
New best validation loss: 2.0907e-04
Epoch 222/300
Validation Loss: 2.0865e-04
New best validation loss: 2.0865e-04
Epoch 223/300
Validation Loss: 2.1147e-04
No improvement in validation loss. Patience: 1/25
Epoch 224/300
Validation Loss: 2.0815e-04
New best validation loss: 2.0815e-04
Epoch 225/300
Validation Loss: 2.0952e-04
No improvement in validation loss. Patience: 1/25
Epoch 226/300
Validation Loss: 2.0644e-04
New best validation loss: 2.0644e-04
Epoch 227/300
Validation Loss: 2.0493e-04
New best validation loss: 2.0493e-04
Epoch 228/300
Validation Loss: 2.0737e-04
No improvement in validation loss. Patience: 1/25
Epoch 229/300
Validation Loss: 2.1676e-04
No improvement in validation loss. Patience: 2/25
Epoch 230/300
Validation Loss: 2.1011e-04
No improvement in validation loss. Patience: 3/25
Epoch 231/300
Validation Loss: 2.1045e-04
No improvement in validation loss. Patience: 4/25
Epoch 232/300
Validation Loss: 2.0786e-04
No improvement in validation loss. Patience: 5/25
Epoch 233/300
Validation Loss: 2.0502e-04
No improvement in validation loss. Patience: 6/25
Epoch 234/300
Validation Loss: 2.0557e-04
No improvement in validation loss. Patience: 7/25
Epoch 235/300
Validation Loss: 2.0866e-04
No improvement in validation loss. Patience: 8/25
Epoch 236/300
Validation Loss: 2.1096e-04
No improvement in validation loss. Patience: 9/25
Epoch 237/300
Validation Loss: 2.0829e-04
No improvement in validation loss. Patience: 10/25
Epoch 238/300
Validation Loss: 2.0695e-04
No improvement in validation loss. Patience: 11/25
Epoch 239/300
Validation Loss: 2.1214e-04
No improvement in validation loss. Patience: 12/25
Epoch 240/300
Validation Loss: 2.0841e-04
No improvement in validation loss. Patience: 13/25
Epoch 241/300
Validation Loss: 2.0513e-04
No improvement in validation loss. Patience: 14/25
Epoch 242/300
Validation Loss: 2.0399e-04
New best validation loss: 2.0399e-04
Epoch 243/300
Validation Loss: 2.0748e-04
No improvement in validation loss. Patience: 1/25
Epoch 244/300
Validation Loss: 2.0408e-04
No improvement in validation loss. Patience: 2/25
Epoch 245/300
Validation Loss: 2.0753e-04
No improvement in validation loss. Patience: 3/25
Epoch 246/300
Validation Loss: 2.0219e-04
New best validation loss: 2.0219e-04
Epoch 247/300
Validation Loss: 2.0208e-04
New best validation loss: 2.0208e-04
Epoch 248/300
Validation Loss: 2.0065e-04
New best validation loss: 2.0065e-04
Epoch 249/300
Validation Loss: 2.0436e-04
No improvement in validation loss. Patience: 1/25
Epoch 250/300
Validation Loss: 2.0242e-04
No improvement in validation loss. Patience: 2/25
Epoch 251/300
Validation Loss: 2.0299e-04
No improvement in validation loss. Patience: 3/25
Epoch 252/300
Validation Loss: 2.0078e-04
No improvement in validation loss. Patience: 4/25
Epoch 253/300
Validation Loss: 1.9999e-04
New best validation loss: 1.9999e-04
Epoch 254/300
Validation Loss: 1.9914e-04
New best validation loss: 1.9914e-04
Epoch 255/300
Validation Loss: 2.0104e-04
No improvement in validation loss. Patience: 1/25
Epoch 256/300
Validation Loss: 2.0140e-04
No improvement in validation loss. Patience: 2/25
Epoch 257/300
Validation Loss: 2.0100e-04
No improvement in validation loss. Patience: 3/25
Epoch 258/300
Validation Loss: 2.0020e-04
No improvement in validation loss. Patience: 4/25
Epoch 259/300
Validation Loss: 1.9905e-04
New best validation loss: 1.9905e-04
Epoch 260/300
Validation Loss: 2.0088e-04
No improvement in validation loss. Patience: 1/25
Epoch 261/300
Validation Loss: 1.9869e-04
New best validation loss: 1.9869e-04
Epoch 262/300
Validation Loss: 1.9889e-04
No improvement in validation loss. Patience: 1/25
Epoch 263/300
Validation Loss: 2.0160e-04
No improvement in validation loss. Patience: 2/25
Epoch 264/300
Validation Loss: 2.0048e-04
No improvement in validation loss. Patience: 3/25
Epoch 265/300
Validation Loss: 1.9949e-04
No improvement in validation loss. Patience: 4/25
Epoch 266/300
Validation Loss: 1.9882e-04
No improvement in validation loss. Patience: 5/25
Epoch 267/300
Validation Loss: 1.9867e-04
New best validation loss: 1.9867e-04
Epoch 268/300
Validation Loss: 2.0075e-04
No improvement in validation loss. Patience: 1/25
Epoch 269/300
Validation Loss: 1.9780e-04
New best validation loss: 1.9780e-04
Epoch 270/300
Validation Loss: 1.9895e-04
No improvement in validation loss. Patience: 1/25
Epoch 271/300
Validation Loss: 1.9872e-04
No improvement in validation loss. Patience: 2/25
Epoch 272/300
Validation Loss: 1.9813e-04
No improvement in validation loss. Patience: 3/25
Epoch 273/300
Validation Loss: 1.9698e-04
New best validation loss: 1.9698e-04
Epoch 274/300
Validation Loss: 1.9750e-04
No improvement in validation loss. Patience: 1/25
Epoch 275/300
Validation Loss: 1.9720e-04
No improvement in validation loss. Patience: 2/25
Epoch 276/300
Validation Loss: 1.9735e-04
No improvement in validation loss. Patience: 3/25
Epoch 277/300
Validation Loss: 1.9733e-04
No improvement in validation loss. Patience: 4/25
Epoch 278/300
Validation Loss: 1.9732e-04
No improvement in validation loss. Patience: 5/25
Epoch 279/300
Validation Loss: 1.9719e-04
No improvement in validation loss. Patience: 6/25
Epoch 280/300
Validation Loss: 1.9767e-04
No improvement in validation loss. Patience: 7/25
Epoch 281/300
Validation Loss: 1.9674e-04
New best validation loss: 1.9674e-04
Epoch 282/300
Validation Loss: 1.9672e-04
New best validation loss: 1.9672e-04
Epoch 283/300
Validation Loss: 1.9680e-04
No improvement in validation loss. Patience: 1/25
Epoch 284/300
Validation Loss: 1.9680e-04
No improvement in validation loss. Patience: 2/25
Epoch 285/300
Validation Loss: 1.9634e-04
New best validation loss: 1.9634e-04
Epoch 286/300
Validation Loss: 1.9641e-04
No improvement in validation loss. Patience: 1/25
Epoch 287/300
Validation Loss: 1.9639e-04
No improvement in validation loss. Patience: 2/25
Epoch 288/300
Validation Loss: 1.9632e-04
New best validation loss: 1.9632e-04
Epoch 289/300
Validation Loss: 1.9644e-04
No improvement in validation loss. Patience: 1/25
Epoch 290/300
Validation Loss: 1.9640e-04
No improvement in validation loss. Patience: 2/25
Epoch 291/300
Validation Loss: 1.9621e-04
New best validation loss: 1.9621e-04
Epoch 292/300
Validation Loss: 1.9620e-04
New best validation loss: 1.9620e-04
Epoch 293/300
Validation Loss: 1.9612e-04
New best validation loss: 1.9612e-04
Epoch 294/300
Validation Loss: 1.9612e-04
No improvement in validation loss. Patience: 1/25
Epoch 295/300
Validation Loss: 1.9616e-04
No improvement in validation loss. Patience: 2/25
Epoch 296/300
Validation Loss: 1.9609e-04
New best validation loss: 1.9609e-04
Epoch 297/300
Validation Loss: 1.9609e-04
No improvement in validation loss. Patience: 1/25
Epoch 298/300
Validation Loss: 1.9609e-04
No improvement in validation loss. Patience: 2/25
Epoch 299/300
Validation Loss: 1.9608e-04
New best validation loss: 1.9608e-04
Epoch 300/300
Validation Loss: 1.9608e-04
New best validation loss: 1.9608e-04
Checkpoint saved at step 300.
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/old-cnn_superbit_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================

Shape of test images: (5000, 53, 53)
Shape of test labels: (5000, 4)
Number of matching directories found: 1
Matching directory 1: old-cnn_superbit_low-noise300
Model checkpoint loaded successfully.

[1m=== Combined Metrics (ShearNet) ===[0m
Mean Squared Error (MSE) from ShearNet: [1m[93m2.322742e-04[0m
Average Bias from ShearNet: [1m[93m-2.082175e-05[0m
Time taken: [1m[96m2.22 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 2.834432e-04, Bias = +1.585014e-04
             g2: MSE = 3.127196e-04, Bias = -4.177294e-05
  g1g2_combined: MSE = 2.980813e-04, Bias = +5.836418e-05
          sigma: MSE = 1.119860e-04, Bias = +1.260446e-04
           flux: MSE = 2.209480e-04, Bias = -3.260599e-04

Starting NGmix ML fitting: num_gal: 5000 | psf_model: gauss | gal_model: gauss | num_cores: 96

[1m=== Combined Metrics (NGmix) ===[0m
Mean Squared Error (MSE) from NGmix: [1m[93m3.006392e-02[0m
Average Bias from NGmix: [1m[93m-5.363363e-02[0m
Time taken: [1m[96m108.54 seconds[0m

=== Per-Label Metrics ===
             g1: MSE = 1.219160e-03, Bias = +1.701112e-03
             g2: MSE = 1.333070e-03, Bias = +1.724779e-04
  g1g2_combined: MSE = 1.276115e-03, Bias = +9.367951e-04
          sigma: MSE = 2.214735e-02, Bias = +1.457259e-01
           flux: MSE = 9.555609e-02, Bias = -3.621340e-01


=== Combined Metrics (Moment-Based Approach) ===
Mean Squared Error (MSE) from MOM: 1.360249e-02
Average Bias from MOM: 7.328964e-03
Time taken: 23.34 seconds

=== Per-Label Metrics ===
             g1: MSE = 1.362008e-02, Bias = +1.577797e-03
             g2: MSE = 1.358489e-02, Bias = +1.308013e-02
  g1g2_combined: MSE = 1.360249e-02, Bias = +7.328964e-03


Generating plots...
Plotting residuals...
Plotting samples...
Plotting scatter plots...

Evaluation complete!

Using config file: configs/deconvnet/research_backed/ideal_psf/normalized/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: True

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_ideal_normalized_high-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_high-noise/training_config.yaml

Epoch 1/300
  Training Loss: 3.610452e-02
  Validation Loss: 2.054410e-02
  New best validation loss: 2.054410e-02

Epoch 2/300
  Training Loss: 2.189103e-02
  Validation Loss: 1.702446e-02
  New best validation loss: 1.702446e-02

Epoch 3/300
  Training Loss: 1.947128e-02
  Validation Loss: 1.509080e-02
  New best validation loss: 1.509080e-02

Epoch 4/300
  Training Loss: 1.819314e-02
  Validation Loss: 1.434288e-02
  New best validation loss: 1.434288e-02

Epoch 5/300
  Training Loss: 1.744342e-02
  Validation Loss: 1.481426e-02
  No improvement. Patience: 1/25

Epoch 6/300
  Training Loss: 1.678439e-02
  Validation Loss: 1.560718e-02
  No improvement. Patience: 2/25

Epoch 7/300
  Training Loss: 1.630064e-02
  Validation Loss: 1.337901e-02
  New best validation loss: 1.337901e-02

Epoch 8/300
  Training Loss: 1.587795e-02
  Validation Loss: 1.442601e-02
  No improvement. Patience: 1/25

Epoch 9/300
  Training Loss: 1.553852e-02
  Validation Loss: 1.354880e-02
  No improvement. Patience: 2/25

Epoch 10/300
  Training Loss: 1.519719e-02
  Validation Loss: 1.420933e-02
  No improvement. Patience: 3/25

Epoch 11/300
  Training Loss: 1.495896e-02
  Validation Loss: 1.341880e-02
  No improvement. Patience: 4/25

Epoch 12/300
  Training Loss: 1.474889e-02
  Validation Loss: 1.547324e-02
  No improvement. Patience: 5/25

Epoch 13/300
  Training Loss: 1.449307e-02
  Validation Loss: 1.469261e-02
  No improvement. Patience: 6/25

Epoch 14/300
  Training Loss: 1.431957e-02
  Validation Loss: 1.692149e-02
  No improvement. Patience: 7/25

Epoch 15/300
  Training Loss: 1.413615e-02
  Validation Loss: 1.569938e-02
  No improvement. Patience: 8/25

Epoch 16/300
  Training Loss: 1.394786e-02
  Validation Loss: 1.657659e-02
  No improvement. Patience: 9/25

Epoch 17/300
  Training Loss: 1.377905e-02
  Validation Loss: 1.610432e-02
  No improvement. Patience: 10/25

Epoch 18/300
  Training Loss: 1.367889e-02
  Validation Loss: 1.605490e-02
  No improvement. Patience: 11/25

Epoch 19/300
  Training Loss: 1.354045e-02
  Validation Loss: 1.950554e-02
  No improvement. Patience: 12/25

Epoch 20/300
  Training Loss: 1.339151e-02
  Validation Loss: 1.860520e-02
  No improvement. Patience: 13/25

Epoch 21/300
  Training Loss: 1.319893e-02
  Validation Loss: 1.993525e-02
  No improvement. Patience: 14/25

Epoch 22/300
  Training Loss: 1.309666e-02
  Validation Loss: 1.811870e-02
  No improvement. Patience: 15/25

Epoch 23/300
  Training Loss: 1.292142e-02
  Validation Loss: 2.331105e-02
  No improvement. Patience: 16/25

Epoch 24/300
  Training Loss: 1.277013e-02
  Validation Loss: 2.369701e-02
  No improvement. Patience: 17/25

Epoch 25/300
  Training Loss: 1.263151e-02
  Validation Loss: 1.754884e-02
  No improvement. Patience: 18/25

Epoch 26/300
  Training Loss: 1.245426e-02
  Validation Loss: 2.407699e-02
  No improvement. Patience: 19/25

Epoch 27/300
  Training Loss: 1.229085e-02
  Validation Loss: 2.353870e-02
  No improvement. Patience: 20/25

Epoch 28/300
  Training Loss: 1.213651e-02
  Validation Loss: 2.158610e-02
  No improvement. Patience: 21/25

Epoch 29/300
  Training Loss: 1.192780e-02
  Validation Loss: 1.768042e-02
  No improvement. Patience: 22/25

Epoch 30/300
  Training Loss: 1.177663e-02
  Validation Loss: 2.003894e-02
  No improvement. Patience: 23/25

Epoch 31/300
  Training Loss: 1.162058e-02
  Validation Loss: 2.267324e-02
  No improvement. Patience: 24/25

Epoch 32/300
  Training Loss: 1.148774e-02
  Validation Loss: 2.206426e-02
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 32
Checkpoint saved at step 32.

 Training completed!
Best validation loss: 1.337901e-02
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (5000, 53, 53)
Test PSF images shape: (5000, 53, 53)
Test target images shape: (5000, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_normalized_high-noise32
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_normalized_high-noise32

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m4.319176e-02[0m
Mean Absolute Error (MAE): [1m[93m7.875383e-02[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m52.83 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.9058[0m
Approximate Perceptual Distance: [1m[96m8.921970e-02[0m
Bias: [1m+1.654817e-03[0m
Normalized MSE: [1m4.319175e-02[0m
Evaluation time: [1m[96m36.41 seconds[0m

[1m=== SANITY CHECKS ===[0m
[91mWARNING: PSNR > 50 dB is unusually high. Check for data leakage or model copying input.[0m
Prediction-Target Correlation: 0.979635

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m20.34 seconds[0m
Mean Squared Error (MSE): [1m[93m6.024284e+00[0m
Mean Absolute Error (MAE): [1m[93m1.778141e+00[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m31.39 dB[0m
Bias: [1m+1.093782e-03[0m

==================================================
Method Comparison
==================================================
MSE                       4.319e-02       6.024e+00       [92mNeural[0m
MAE                       7.875e-02       1.778e+00       [92mNeural[0m
PSNR                      52.83           31.39           [92mNeural[0m
SSIM                      0.9058          0.0350          [92mNeural[0m
NORMALIZED_MSE            4.319e-02       6.024e+00       [92mNeural[0m
TIME_TAKEN                36.41s          20.34s          [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (5000, 53, 53)
GalSim predictions shape: (5000, 53, 53)
Target images shape: (5000, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_high-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 5000 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_high-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_high-noise

Evaluation complete!

Using config file: configs/deconvnet/research_backed/ideal_psf/normalized/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: True

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_ideal_normalized_low-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_low-noise/training_config.yaml

Epoch 1/300
  Training Loss: 8.136273e-03
  Validation Loss: 3.199740e-03
  New best validation loss: 3.199740e-03

Epoch 2/300
  Training Loss: 3.481767e-03
  Validation Loss: 1.802496e-03
  New best validation loss: 1.802496e-03

Epoch 3/300
  Training Loss: 2.376651e-03
  Validation Loss: 1.604479e-03
  New best validation loss: 1.604479e-03

Epoch 4/300
  Training Loss: 1.980889e-03
  Validation Loss: 1.655367e-03
  No improvement. Patience: 1/25

Epoch 5/300
  Training Loss: 1.692534e-03
  Validation Loss: 1.239238e-03
  New best validation loss: 1.239238e-03

Epoch 6/300
  Training Loss: 1.486714e-03
  Validation Loss: 1.470066e-03
  No improvement. Patience: 1/25

Epoch 7/300
  Training Loss: 1.341569e-03
  Validation Loss: 9.380459e-04
  New best validation loss: 9.380459e-04

Epoch 8/300
  Training Loss: 1.250493e-03
  Validation Loss: 4.993248e-04
  New best validation loss: 4.993248e-04

Epoch 9/300
  Training Loss: 1.162626e-03
  Validation Loss: 4.102423e-04
  New best validation loss: 4.102423e-04

Epoch 10/300
  Training Loss: 1.106768e-03
  Validation Loss: 9.399363e-04
  No improvement. Patience: 1/25

Epoch 11/300
  Training Loss: 1.040717e-03
  Validation Loss: 5.355767e-04
  No improvement. Patience: 2/25

Epoch 12/300
  Training Loss: 9.767124e-04
  Validation Loss: 6.934880e-04
  No improvement. Patience: 3/25

Epoch 13/300
  Training Loss: 9.250951e-04
  Validation Loss: 5.055044e-04
  No improvement. Patience: 4/25

Epoch 14/300
  Training Loss: 9.086109e-04
  Validation Loss: 3.367561e-04
  New best validation loss: 3.367561e-04

Epoch 15/300
  Training Loss: 8.569414e-04
  Validation Loss: 3.194843e-04
  New best validation loss: 3.194843e-04

Epoch 16/300
  Training Loss: 8.279872e-04
  Validation Loss: 4.828950e-04
  No improvement. Patience: 1/25

Epoch 17/300
  Training Loss: 7.778823e-04
  Validation Loss: 3.853146e-04
  No improvement. Patience: 2/25

Epoch 18/300
  Training Loss: 7.586820e-04
  Validation Loss: 5.122029e-04
  No improvement. Patience: 3/25

Epoch 19/300
  Training Loss: 7.338850e-04
  Validation Loss: 2.169283e-04
  New best validation loss: 2.169283e-04

Epoch 20/300
  Training Loss: 6.871872e-04
  Validation Loss: 2.937739e-04
  No improvement. Patience: 1/25

Epoch 21/300
  Training Loss: 7.035784e-04
  Validation Loss: 2.587341e-04
  No improvement. Patience: 2/25

Epoch 22/300
  Training Loss: 6.418214e-04
  Validation Loss: 1.078605e-03
  No improvement. Patience: 3/25

Epoch 23/300
  Training Loss: 6.430674e-04
  Validation Loss: 4.666531e-04
  No improvement. Patience: 4/25

Epoch 24/300
  Training Loss: 6.027619e-04
  Validation Loss: 4.698384e-04
  No improvement. Patience: 5/25

Epoch 25/300
  Training Loss: 6.229035e-04
  Validation Loss: 5.458752e-04
  No improvement. Patience: 6/25

Epoch 26/300
  Training Loss: 5.748009e-04
  Validation Loss: 3.449591e-04
  No improvement. Patience: 7/25

Epoch 27/300
  Training Loss: 6.073245e-04
  Validation Loss: 4.118705e-04
  No improvement. Patience: 8/25

Epoch 28/300
  Training Loss: 5.442076e-04
  Validation Loss: 3.838277e-04
  No improvement. Patience: 9/25

Epoch 29/300
  Training Loss: 5.618059e-04
  Validation Loss: 5.231752e-04
  No improvement. Patience: 10/25

Epoch 30/300
  Training Loss: 5.291611e-04
  Validation Loss: 4.823971e-04
  No improvement. Patience: 11/25

Epoch 31/300
  Training Loss: 5.335690e-04
  Validation Loss: 4.074537e-04
  No improvement. Patience: 12/25

Epoch 32/300
  Training Loss: 5.222232e-04
  Validation Loss: 5.736815e-04
  No improvement. Patience: 13/25

Epoch 33/300
  Training Loss: 4.908213e-04
  Validation Loss: 7.937101e-04
  No improvement. Patience: 14/25

Epoch 34/300
  Training Loss: 5.053161e-04
  Validation Loss: 7.865072e-04
  No improvement. Patience: 15/25

Epoch 35/300
  Training Loss: 4.922456e-04
  Validation Loss: 5.247187e-04
  No improvement. Patience: 16/25

Epoch 36/300
  Training Loss: 4.903024e-04
  Validation Loss: 1.955166e-03
  No improvement. Patience: 17/25

Epoch 37/300
  Training Loss: 4.712418e-04
  Validation Loss: 4.197722e-04
  No improvement. Patience: 18/25

Epoch 38/300
  Training Loss: 4.507168e-04
  Validation Loss: 4.375422e-04
  No improvement. Patience: 19/25

Epoch 39/300
  Training Loss: 4.552251e-04
  Validation Loss: 7.765574e-04
  No improvement. Patience: 20/25

Epoch 40/300
  Training Loss: 4.461192e-04
  Validation Loss: 5.217830e-04
  No improvement. Patience: 21/25

Epoch 41/300
  Training Loss: 4.305514e-04
  Validation Loss: 1.048305e-03
  No improvement. Patience: 22/25

Epoch 42/300
  Training Loss: 4.422894e-04
  Validation Loss: 8.309665e-04
  No improvement. Patience: 23/25

Epoch 43/300
  Training Loss: 4.367965e-04
  Validation Loss: 3.745690e-04
  No improvement. Patience: 24/25

Epoch 44/300
  Training Loss: 4.199351e-04
  Validation Loss: 9.047968e-04
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 44
Checkpoint saved at step 44.

 Training completed!
Best validation loss: 2.169283e-04
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (5000, 53, 53)
Test PSF images shape: (5000, 53, 53)
Test target images shape: (5000, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_normalized_low-noise44
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_normalized_low-noise44

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m1.642471e-03[0m
Mean Absolute Error (MAE): [1m[93m5.909460e-03[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m66.58 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.9985[0m
Approximate Perceptual Distance: [1m[96m3.917493e-03[0m
Bias: [1m+2.646546e-03[0m
Normalized MSE: [1m1.642471e-03[0m
Evaluation time: [1m[96m36.68 seconds[0m

[1m=== SANITY CHECKS ===[0m
[91mWARNING: PSNR > 50 dB is unusually high. Check for data leakage or model copying input.[0m
[91mWARNING: SSIM > 0.99 is unusually high. Model may be learning to copy rather than deconvolve.[0m
Prediction-Target Correlation: 0.998804

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m20.72 seconds[0m
Mean Squared Error (MSE): [1m[93m9.851233e-01[0m
Mean Absolute Error (MAE): [1m[93m2.463702e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m38.80 dB[0m
Bias: [1m+1.089095e-03[0m

==================================================
Method Comparison
==================================================
MSE                       1.642e-03       9.851e-01       [92mNeural[0m
MAE                       5.909e-03       2.464e-01       [92mNeural[0m
PSNR                      66.58           38.80           [92mNeural[0m
SSIM                      0.9985          0.1034          [92mNeural[0m
NORMALIZED_MSE            1.642e-03       9.851e-01       [92mNeural[0m
TIME_TAKEN                36.68s          20.72s          [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (5000, 53, 53)
GalSim predictions shape: (5000, 53, 53)
Target images shape: (5000, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_low-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 5000 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_low-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_ideal_normalized_low-noise

Evaluation complete!

Using config file: configs/deconvnet/research_backed/ideal_psf/not_normalized/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: False

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_ideal_not-normalized_high-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_high-noise/training_config.yaml

Epoch 1/300
  Training Loss: 2.247506e-05
  Validation Loss: 3.910301e-06
  New best validation loss: 3.910301e-06

Epoch 2/300
  Training Loss: 1.689070e-06
  Validation Loss: 2.254613e-06
  New best validation loss: 2.254613e-06

Epoch 3/300
  Training Loss: 1.384285e-06
  Validation Loss: 1.872694e-06
  New best validation loss: 1.872694e-06

Epoch 4/300
  Training Loss: 1.274277e-06
  Validation Loss: 1.614648e-06
  New best validation loss: 1.614648e-06

Epoch 5/300
  Training Loss: 1.211354e-06
  Validation Loss: 1.703444e-06
  No improvement. Patience: 1/25

Epoch 6/300
  Training Loss: 1.161206e-06
  Validation Loss: 1.527667e-06
  New best validation loss: 1.527667e-06

Epoch 7/300
  Training Loss: 1.120782e-06
  Validation Loss: 1.568794e-06
  No improvement. Patience: 1/25

Epoch 8/300
  Training Loss: 1.083715e-06
  Validation Loss: 1.148260e-06
  New best validation loss: 1.148260e-06

Epoch 9/300
  Training Loss: 1.061793e-06
  Validation Loss: 1.058405e-06
  New best validation loss: 1.058405e-06

Epoch 10/300
  Training Loss: 1.032714e-06
  Validation Loss: 1.291604e-06
  No improvement. Patience: 1/25

Epoch 11/300
  Training Loss: 1.009724e-06
  Validation Loss: 1.165344e-06
  No improvement. Patience: 2/25

Epoch 12/300
  Training Loss: 9.928912e-07
  Validation Loss: 9.106950e-07
  New best validation loss: 9.106950e-07

Epoch 13/300
  Training Loss: 9.712652e-07
  Validation Loss: 9.823723e-07
  No improvement. Patience: 1/25

Epoch 14/300
  Training Loss: 9.564894e-07
  Validation Loss: 8.014043e-07
  New best validation loss: 8.014043e-07

Epoch 15/300
  Training Loss: 9.386686e-07
  Validation Loss: 7.642089e-07
  New best validation loss: 7.642089e-07

Epoch 16/300
  Training Loss: 9.259114e-07
  Validation Loss: 1.018147e-06
  No improvement. Patience: 1/25

Epoch 17/300
  Training Loss: 9.146722e-07
  Validation Loss: 8.038924e-07
  No improvement. Patience: 2/25

Epoch 18/300
  Training Loss: 8.997367e-07
  Validation Loss: 1.017270e-06
  No improvement. Patience: 3/25

Epoch 19/300
  Training Loss: 8.872601e-07
  Validation Loss: 8.456523e-07
  No improvement. Patience: 4/25

Epoch 20/300
  Training Loss: 8.793646e-07
  Validation Loss: 9.392355e-07
  No improvement. Patience: 5/25

Epoch 21/300
  Training Loss: 8.634254e-07
  Validation Loss: 8.916358e-07
  No improvement. Patience: 6/25

Epoch 22/300
  Training Loss: 8.501929e-07
  Validation Loss: 8.524683e-07
  No improvement. Patience: 7/25

Epoch 23/300
  Training Loss: 8.374988e-07
  Validation Loss: 8.782167e-07
  No improvement. Patience: 8/25

Epoch 24/300
  Training Loss: 8.199876e-07
  Validation Loss: 8.487453e-07
  No improvement. Patience: 9/25

Epoch 25/300
  Training Loss: 8.006040e-07
  Validation Loss: 9.333573e-07
  No improvement. Patience: 10/25

Epoch 26/300
  Training Loss: 7.845775e-07
  Validation Loss: 9.823119e-07
  No improvement. Patience: 11/25

Epoch 27/300
  Training Loss: 7.669482e-07
  Validation Loss: 8.666388e-07
  No improvement. Patience: 12/25

Epoch 28/300
  Training Loss: 7.516740e-07
  Validation Loss: 9.380161e-07
  No improvement. Patience: 13/25

Epoch 29/300
  Training Loss: 7.351279e-07
  Validation Loss: 9.905150e-07
  No improvement. Patience: 14/25

Epoch 30/300
  Training Loss: 7.214782e-07
  Validation Loss: 1.064980e-06
  No improvement. Patience: 15/25

Epoch 31/300
  Training Loss: 7.042811e-07
  Validation Loss: 1.090703e-06
  No improvement. Patience: 16/25

Epoch 32/300
  Training Loss: 6.844320e-07
  Validation Loss: 1.019328e-06
  No improvement. Patience: 17/25

Epoch 33/300
  Training Loss: 6.646079e-07
  Validation Loss: 1.132032e-06
  No improvement. Patience: 18/25

Epoch 34/300
  Training Loss: 6.464085e-07
  Validation Loss: 1.144360e-06
  No improvement. Patience: 19/25

Epoch 35/300
  Training Loss: 6.279216e-07
  Validation Loss: 1.054548e-06
  No improvement. Patience: 20/25

Epoch 36/300
  Training Loss: 6.170530e-07
  Validation Loss: 1.207145e-06
  No improvement. Patience: 21/25

Epoch 37/300
  Training Loss: 5.995208e-07
  Validation Loss: 1.207558e-06
  No improvement. Patience: 22/25

Epoch 38/300
  Training Loss: 5.802028e-07
  Validation Loss: 1.188006e-06
  No improvement. Patience: 23/25

Epoch 39/300
  Training Loss: 5.666583e-07
  Validation Loss: 1.331065e-06
  No improvement. Patience: 24/25

Epoch 40/300
  Training Loss: 5.483037e-07
  Validation Loss: 1.417725e-06
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 40
Checkpoint saved at step 40.

 Training completed!
Best validation loss: 7.642089e-07
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (5000, 53, 53)
Test PSF images shape: (5000, 53, 53)
Test target images shape: (5000, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_not-normalized_high-noise40
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_not-normalized_high-noise40

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m1.183654e+00[0m
Mean Absolute Error (MAE): [1m[93m7.452657e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m38.29 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.3281[0m
Approximate Perceptual Distance: [1m[96m2.917214e+00[0m
Bias: [1m-3.401327e-04[0m
Normalized MSE: [1m1.183654e+00[0m
Evaluation time: [1m[96m34.67 seconds[0m

[1m=== SANITY CHECKS ===[0m
Prediction-Target Correlation: 0.404474

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m20.52 seconds[0m
Mean Squared Error (MSE): [1m[93m6.024373e+00[0m
Mean Absolute Error (MAE): [1m[93m1.777823e+00[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m31.22 dB[0m
Bias: [1m+1.089339e-03[0m

==================================================
Method Comparison
==================================================
MSE                       1.184e+00       6.024e+00       [92mNeural[0m
MAE                       7.453e-01       1.778e+00       [92mNeural[0m
PSNR                      38.29           31.22           [92mNeural[0m
SSIM                      0.3281          0.0350          [92mNeural[0m
NORMALIZED_MSE            1.184e+00       6.024e+00       [92mNeural[0m
TIME_TAKEN                34.67s          20.52s          [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (5000, 53, 53)
GalSim predictions shape: (5000, 53, 53)
Target images shape: (5000, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_high-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 5000 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_high-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_high-noise

Evaluation complete!

Using config file: configs/deconvnet/research_backed/ideal_psf/not_normalized/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: ideal
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: False

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_ideal_not-normalized_low-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_low-noise/training_config.yaml

Epoch 1/300
  Training Loss: 1.850329e-05
  Validation Loss: 3.620110e-07
  New best validation loss: 3.620110e-07

Epoch 2/300
  Training Loss: 3.534080e-07
  Validation Loss: 3.718312e-07
  No improvement. Patience: 1/25

Epoch 3/300
  Training Loss: 2.232331e-07
  Validation Loss: 1.881804e-07
  New best validation loss: 1.881804e-07

Epoch 4/300
  Training Loss: 1.698599e-07
  Validation Loss: 1.579733e-07
  New best validation loss: 1.579733e-07

Epoch 5/300
  Training Loss: 1.364442e-07
  Validation Loss: 2.274606e-07
  No improvement. Patience: 1/25

Epoch 6/300
  Training Loss: 1.151699e-07
  Validation Loss: 1.458082e-07
  New best validation loss: 1.458082e-07

Epoch 7/300
  Training Loss: 1.045689e-07
  Validation Loss: 1.826585e-07
  No improvement. Patience: 1/25

Epoch 8/300
  Training Loss: 9.505504e-08
  Validation Loss: 2.223308e-07
  No improvement. Patience: 2/25

Epoch 9/300
  Training Loss: 8.413275e-08
  Validation Loss: 2.810415e-07
  No improvement. Patience: 3/25

Epoch 10/300
  Training Loss: 7.656042e-08
  Validation Loss: 1.163247e-07
  New best validation loss: 1.163247e-07

Epoch 11/300
  Training Loss: 7.213800e-08
  Validation Loss: 1.187267e-07
  No improvement. Patience: 1/25

Epoch 12/300
  Training Loss: 7.252589e-08
  Validation Loss: 8.753744e-08
  New best validation loss: 8.753744e-08

Epoch 13/300
  Training Loss: 6.704894e-08
  Validation Loss: 1.290761e-07
  No improvement. Patience: 1/25

Epoch 14/300
  Training Loss: 6.226213e-08
  Validation Loss: 1.651614e-07
  No improvement. Patience: 2/25

Epoch 15/300
  Training Loss: 5.978293e-08
  Validation Loss: 1.242649e-07
  No improvement. Patience: 3/25

Epoch 16/300
  Training Loss: 5.780842e-08
  Validation Loss: 1.839308e-07
  No improvement. Patience: 4/25

Epoch 17/300
  Training Loss: 5.399128e-08
  Validation Loss: 1.194461e-07
  No improvement. Patience: 5/25

Epoch 18/300
  Training Loss: 5.271371e-08
  Validation Loss: 2.652747e-07
  No improvement. Patience: 6/25

Epoch 19/300
  Training Loss: 5.054826e-08
  Validation Loss: 8.251061e-08
  New best validation loss: 8.251061e-08

Epoch 20/300
  Training Loss: 4.757751e-08
  Validation Loss: 1.580027e-07
  No improvement. Patience: 1/25

Epoch 21/300
  Training Loss: 5.045246e-08
  Validation Loss: 1.440439e-07
  No improvement. Patience: 2/25

Epoch 22/300
  Training Loss: 4.513539e-08
  Validation Loss: 1.398246e-07
  No improvement. Patience: 3/25

Epoch 23/300
  Training Loss: 4.395127e-08
  Validation Loss: 1.120305e-07
  No improvement. Patience: 4/25

Epoch 24/300
  Training Loss: 4.357467e-08
  Validation Loss: 1.328555e-07
  No improvement. Patience: 5/25

Epoch 25/300
  Training Loss: 4.236703e-08
  Validation Loss: 2.179217e-07
  No improvement. Patience: 6/25

Epoch 26/300
  Training Loss: 4.060574e-08
  Validation Loss: 1.609810e-07
  No improvement. Patience: 7/25

Epoch 27/300
  Training Loss: 4.076251e-08
  Validation Loss: 2.546244e-07
  No improvement. Patience: 8/25

Epoch 28/300
  Training Loss: 3.930542e-08
  Validation Loss: 1.290411e-07
  No improvement. Patience: 9/25

Epoch 29/300
  Training Loss: 3.891019e-08
  Validation Loss: 1.119856e-07
  No improvement. Patience: 10/25

Epoch 30/300
  Training Loss: 3.781676e-08
  Validation Loss: 2.417029e-07
  No improvement. Patience: 11/25

Epoch 31/300
  Training Loss: 3.556103e-08
  Validation Loss: 1.702570e-07
  No improvement. Patience: 12/25

Epoch 32/300
  Training Loss: 3.621295e-08
  Validation Loss: 1.377402e-07
  No improvement. Patience: 13/25

Epoch 33/300
  Training Loss: 3.436774e-08
  Validation Loss: 1.297691e-07
  No improvement. Patience: 14/25

Epoch 34/300
  Training Loss: 3.482295e-08
  Validation Loss: 1.996069e-07
  No improvement. Patience: 15/25

Epoch 35/300
  Training Loss: 3.291626e-08
  Validation Loss: 1.669649e-07
  No improvement. Patience: 16/25

Epoch 36/300
  Training Loss: 3.362690e-08
  Validation Loss: 1.594029e-07
  No improvement. Patience: 17/25

Epoch 37/300
  Training Loss: 3.211632e-08
  Validation Loss: 3.855538e-07
  No improvement. Patience: 18/25

Epoch 38/300
  Training Loss: 3.094235e-08
  Validation Loss: 1.612882e-07
  No improvement. Patience: 19/25

Epoch 39/300
  Training Loss: 3.024938e-08
  Validation Loss: 1.219040e-07
  No improvement. Patience: 20/25

Epoch 40/300
  Training Loss: 3.054765e-08
  Validation Loss: 2.285498e-07
  No improvement. Patience: 21/25

Epoch 41/300
  Training Loss: 2.905358e-08
  Validation Loss: 2.141145e-07
  No improvement. Patience: 22/25

Epoch 42/300
  Training Loss: 2.854681e-08
  Validation Loss: 1.846990e-07
  No improvement. Patience: 23/25

Epoch 43/300
  Training Loss: 2.891499e-08
  Validation Loss: 4.764470e-07
  No improvement. Patience: 24/25

Epoch 44/300
  Training Loss: 2.642459e-08
  Validation Loss: 2.791707e-07
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 44
Checkpoint saved at step 44.

 Training completed!
Best validation loss: 8.251061e-08
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 50
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (50, 53, 53)
Test PSF images shape: (50, 53, 53)
Test target images shape: (50, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_not-normalized_low-noise44
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_ideal_not-normalized_low-noise44

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m2.785598e-01[0m
Mean Absolute Error (MAE): [1m[93m1.379270e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m42.71 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.8795[0m
Approximate Perceptual Distance: [1m[96m4.335498e-01[0m
Bias: [1m-1.864690e-05[0m
Normalized MSE: [1m2.785598e-01[0m
Evaluation time: [1m[96m15.29 seconds[0m

[1m=== SANITY CHECKS ===[0m
Prediction-Target Correlation: 0.860890

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m1.36 seconds[0m
Mean Squared Error (MSE): [1m[93m9.853218e-01[0m
Mean Absolute Error (MAE): [1m[93m2.359713e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m37.22 dB[0m
Bias: [1m+1.045867e-03[0m

==================================================
Method Comparison
==================================================
MSE                       2.786e-01       9.853e-01       [92mNeural[0m
MAE                       1.379e-01       2.360e-01       [92mNeural[0m
PSNR                      42.71           37.22           [92mNeural[0m
SSIM                      0.8795          0.0990          [92mNeural[0m
NORMALIZED_MSE            2.786e-01       9.853e-01       [92mNeural[0m
TIME_TAKEN                15.29s          1.36s           [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (50, 53, 53)
GalSim predictions shape: (50, 53, 53)
Target images shape: (50, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_low-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 50 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_low-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_ideal_not-normalized_low-noise

Evaluation complete!

Using config file: configs/deconvnet/research_backed/superbit_psf/normalized/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: True

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_superbit_normalized_high-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_high-noise/training_config.yaml

Epoch 1/300
  Training Loss: 4.782602e-02
  Validation Loss: 2.800614e-02
  New best validation loss: 2.800614e-02

Epoch 2/300
  Training Loss: 3.026969e-02
  Validation Loss: 2.315033e-02
  New best validation loss: 2.315033e-02

Epoch 3/300
  Training Loss: 2.677198e-02
  Validation Loss: 2.141268e-02
  New best validation loss: 2.141268e-02

Epoch 4/300
  Training Loss: 2.513948e-02
  Validation Loss: 2.052742e-02
  New best validation loss: 2.052742e-02

Epoch 5/300
  Training Loss: 2.410803e-02
  Validation Loss: 2.146848e-02
  No improvement. Patience: 1/25

Epoch 6/300
  Training Loss: 2.328589e-02
  Validation Loss: 2.058923e-02
  No improvement. Patience: 2/25

Epoch 7/300
  Training Loss: 2.253558e-02
  Validation Loss: 1.904080e-02
  New best validation loss: 1.904080e-02

Epoch 8/300
  Training Loss: 2.207939e-02
  Validation Loss: 1.974459e-02
  No improvement. Patience: 1/25

Epoch 9/300
  Training Loss: 2.167754e-02
  Validation Loss: 2.025376e-02
  No improvement. Patience: 2/25

Epoch 10/300
  Training Loss: 2.122946e-02
  Validation Loss: 2.022323e-02
  No improvement. Patience: 3/25

Epoch 11/300
  Training Loss: 2.102940e-02
  Validation Loss: 1.855407e-02
  New best validation loss: 1.855407e-02

Epoch 12/300
  Training Loss: 2.065791e-02
  Validation Loss: 1.959892e-02
  No improvement. Patience: 1/25

Epoch 13/300
  Training Loss: 2.037715e-02
  Validation Loss: 2.005659e-02
  No improvement. Patience: 2/25

Epoch 14/300
  Training Loss: 2.017216e-02
  Validation Loss: 2.227426e-02
  No improvement. Patience: 3/25

Epoch 15/300
  Training Loss: 1.988444e-02
  Validation Loss: 2.090810e-02
  No improvement. Patience: 4/25

Epoch 16/300
  Training Loss: 1.972029e-02
  Validation Loss: 2.055559e-02
  No improvement. Patience: 5/25

Epoch 17/300
  Training Loss: 1.948125e-02
  Validation Loss: 2.214766e-02
  No improvement. Patience: 6/25

Epoch 18/300
  Training Loss: 1.922711e-02
  Validation Loss: 1.944674e-02
  No improvement. Patience: 7/25

Epoch 19/300
  Training Loss: 1.897449e-02
  Validation Loss: 2.253225e-02
  No improvement. Patience: 8/25

Epoch 20/300
  Training Loss: 1.882918e-02
  Validation Loss: 2.061726e-02
  No improvement. Patience: 9/25

Epoch 21/300
  Training Loss: 1.858102e-02
  Validation Loss: 2.433250e-02
  No improvement. Patience: 10/25

Epoch 22/300
  Training Loss: 1.832698e-02
  Validation Loss: 2.110165e-02
  No improvement. Patience: 11/25

Epoch 23/300
  Training Loss: 1.817325e-02
  Validation Loss: 2.495623e-02
  No improvement. Patience: 12/25

Epoch 24/300
  Training Loss: 1.793212e-02
  Validation Loss: 2.513352e-02
  No improvement. Patience: 13/25

Epoch 25/300
  Training Loss: 1.774849e-02
  Validation Loss: 2.319296e-02
  No improvement. Patience: 14/25

Epoch 26/300
  Training Loss: 1.752415e-02
  Validation Loss: 2.982559e-02
  No improvement. Patience: 15/25

Epoch 27/300
  Training Loss: 1.736241e-02
  Validation Loss: 2.873697e-02
  No improvement. Patience: 16/25

Epoch 28/300
  Training Loss: 1.716418e-02
  Validation Loss: 2.439225e-02
  No improvement. Patience: 17/25

Epoch 29/300
  Training Loss: 1.695331e-02
  Validation Loss: 2.150668e-02
  No improvement. Patience: 18/25

Epoch 30/300
  Training Loss: 1.671411e-02
  Validation Loss: 2.429935e-02
  No improvement. Patience: 19/25

Epoch 31/300
  Training Loss: 1.655088e-02
  Validation Loss: 2.546045e-02
  No improvement. Patience: 20/25

Epoch 32/300
  Training Loss: 1.640329e-02
  Validation Loss: 2.844499e-02
  No improvement. Patience: 21/25

Epoch 33/300
  Training Loss: 1.613843e-02
  Validation Loss: 2.415722e-02
  No improvement. Patience: 22/25

Epoch 34/300
  Training Loss: 1.597944e-02
  Validation Loss: 2.448069e-02
  No improvement. Patience: 23/25

Epoch 35/300
  Training Loss: 1.580449e-02
  Validation Loss: 2.827564e-02
  No improvement. Patience: 24/25

Epoch 36/300
  Training Loss: 1.564763e-02
  Validation Loss: 2.509422e-02
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 36
Checkpoint saved at step 36.

 Training completed!
Best validation loss: 1.855407e-02
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_high-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (5000, 53, 53)
Test PSF images shape: (5000, 53, 53)
Test target images shape: (5000, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_superbit_normalized_high-noise36
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_superbit_normalized_high-noise36

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m4.787259e-02[0m
Mean Absolute Error (MAE): [1m[93m8.774653e-02[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m52.73 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.8933[0m
Approximate Perceptual Distance: [1m[96m9.741431e-02[0m
Bias: [1m-6.215517e-03[0m
Normalized MSE: [1m4.787259e-02[0m
Evaluation time: [1m[96m36.12 seconds[0m

[1m=== SANITY CHECKS ===[0m
[91mWARNING: PSNR > 50 dB is unusually high. Check for data leakage or model copying input.[0m
Prediction-Target Correlation: 0.975896

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m20.30 seconds[0m
Mean Squared Error (MSE): [1m[93m1.876324e+00[0m
Mean Absolute Error (MAE): [1m[93m6.511730e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m36.80 dB[0m
Bias: [1m+1.108173e-03[0m

==================================================
Method Comparison
==================================================
MSE                       4.787e-02       1.876e+00       [92mNeural[0m
MAE                       8.775e-02       6.512e-01       [92mNeural[0m
PSNR                      52.73           36.80           [92mNeural[0m
SSIM                      0.8933          0.0803          [92mNeural[0m
NORMALIZED_MSE            4.787e-02       1.876e+00       [92mNeural[0m
TIME_TAKEN                36.12s          20.30s          [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (5000, 53, 53)
GalSim predictions shape: (5000, 53, 53)
Target images shape: (5000, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_high-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 5000 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_high-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_high-noise

Evaluation complete!

Using config file: configs/deconvnet/research_backed/superbit_psf/normalized/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: True

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_superbit_normalized_low-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_low-noise/training_config.yaml

Epoch 1/300
  Training Loss: 1.032349e-02
  Validation Loss: 3.072652e-03
  New best validation loss: 3.072652e-03

Epoch 2/300
  Training Loss: 4.354368e-03
  Validation Loss: 4.584712e-03
  No improvement. Patience: 1/25

Epoch 3/300
  Training Loss: 3.303824e-03
  Validation Loss: 3.041091e-03
  New best validation loss: 3.041091e-03

Epoch 4/300
  Training Loss: 2.740914e-03
  Validation Loss: 1.356556e-03
  New best validation loss: 1.356556e-03

Epoch 5/300
  Training Loss: 2.423306e-03
  Validation Loss: 1.557170e-03
  No improvement. Patience: 1/25

Epoch 6/300
  Training Loss: 2.216567e-03
  Validation Loss: 1.485976e-03
  No improvement. Patience: 2/25

Epoch 7/300
  Training Loss: 2.012674e-03
  Validation Loss: 1.011913e-03
  New best validation loss: 1.011913e-03

Epoch 8/300
  Training Loss: 1.878454e-03
  Validation Loss: 1.464830e-03
  No improvement. Patience: 1/25

Epoch 9/300
  Training Loss: 1.778242e-03
  Validation Loss: 1.384519e-03
  No improvement. Patience: 2/25

Epoch 10/300
  Training Loss: 1.657422e-03
  Validation Loss: 1.715790e-03
  No improvement. Patience: 3/25

Epoch 11/300
  Training Loss: 1.549421e-03
  Validation Loss: 1.439579e-03
  No improvement. Patience: 4/25

Epoch 12/300
  Training Loss: 1.538174e-03
  Validation Loss: 8.449311e-04
  New best validation loss: 8.449311e-04

Epoch 13/300
  Training Loss: 1.447859e-03
  Validation Loss: 9.489834e-04
  No improvement. Patience: 1/25

Epoch 14/300
  Training Loss: 1.362534e-03
  Validation Loss: 8.123645e-04
  New best validation loss: 8.123645e-04

Epoch 15/300
  Training Loss: 1.337209e-03
  Validation Loss: 1.212246e-03
  No improvement. Patience: 1/25

Epoch 16/300
  Training Loss: 1.286063e-03
  Validation Loss: 1.560752e-03
  No improvement. Patience: 2/25

Epoch 17/300
  Training Loss: 1.245938e-03
  Validation Loss: 1.549808e-03
  No improvement. Patience: 3/25

Epoch 18/300
  Training Loss: 1.201263e-03
  Validation Loss: 1.116978e-03
  No improvement. Patience: 4/25

Epoch 19/300
  Training Loss: 1.190094e-03
  Validation Loss: 1.292702e-03
  No improvement. Patience: 5/25

Epoch 20/300
  Training Loss: 1.140575e-03
  Validation Loss: 8.324771e-04
  No improvement. Patience: 6/25

Epoch 21/300
  Training Loss: 1.112984e-03
  Validation Loss: 6.635021e-04
  New best validation loss: 6.635021e-04

Epoch 22/300
  Training Loss: 1.093746e-03
  Validation Loss: 9.294140e-04
  No improvement. Patience: 1/25

Epoch 23/300
  Training Loss: 1.046861e-03
  Validation Loss: 1.074876e-03
  No improvement. Patience: 2/25

Epoch 24/300
  Training Loss: 1.058347e-03
  Validation Loss: 1.226546e-03
  No improvement. Patience: 3/25

Epoch 25/300
  Training Loss: 1.012451e-03
  Validation Loss: 7.560882e-04
  No improvement. Patience: 4/25

Epoch 26/300
  Training Loss: 1.004365e-03
  Validation Loss: 1.690266e-03
  No improvement. Patience: 5/25

Epoch 27/300
  Training Loss: 9.627141e-04
  Validation Loss: 1.124883e-03
  No improvement. Patience: 6/25

Epoch 28/300
  Training Loss: 9.235516e-04
  Validation Loss: 6.933234e-04
  No improvement. Patience: 7/25

Epoch 29/300
  Training Loss: 9.373369e-04
  Validation Loss: 1.026515e-03
  No improvement. Patience: 8/25

Epoch 30/300
  Training Loss: 9.050759e-04
  Validation Loss: 1.031122e-03
  No improvement. Patience: 9/25

Epoch 31/300
  Training Loss: 8.778381e-04
  Validation Loss: 1.483064e-03
  No improvement. Patience: 10/25

Epoch 32/300
  Training Loss: 8.771957e-04
  Validation Loss: 1.541959e-03
  No improvement. Patience: 11/25

Epoch 33/300
  Training Loss: 8.492731e-04
  Validation Loss: 8.156569e-04
  No improvement. Patience: 12/25

Epoch 34/300
  Training Loss: 8.387292e-04
  Validation Loss: 1.584532e-03
  No improvement. Patience: 13/25

Epoch 35/300
  Training Loss: 8.337134e-04
  Validation Loss: 1.224163e-03
  No improvement. Patience: 14/25

Epoch 36/300
  Training Loss: 8.109670e-04
  Validation Loss: 1.545819e-03
  No improvement. Patience: 15/25

Epoch 37/300
  Training Loss: 7.904996e-04
  Validation Loss: 1.080963e-03
  No improvement. Patience: 16/25

Epoch 38/300
  Training Loss: 7.925339e-04
  Validation Loss: 1.301753e-03
  No improvement. Patience: 17/25

Epoch 39/300
  Training Loss: 7.649384e-04
  Validation Loss: 8.228141e-04
  No improvement. Patience: 18/25

Epoch 40/300
  Training Loss: 7.554677e-04
  Validation Loss: 2.123402e-03
  No improvement. Patience: 19/25

Epoch 41/300
  Training Loss: 7.594996e-04
  Validation Loss: 1.618239e-03
  No improvement. Patience: 20/25

Epoch 42/300
  Training Loss: 7.085409e-04
  Validation Loss: 3.252486e-03
  No improvement. Patience: 21/25

Epoch 43/300
  Training Loss: 7.149768e-04
  Validation Loss: 1.136660e-03
  No improvement. Patience: 22/25

Epoch 44/300
  Training Loss: 7.132867e-04
  Validation Loss: 1.480826e-03
  No improvement. Patience: 23/25

Epoch 45/300
  Training Loss: 7.075053e-04
  Validation Loss: 1.238097e-03
  No improvement. Patience: 24/25

Epoch 46/300
  Training Loss: 6.913849e-04
  Validation Loss: 1.496276e-03
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 46
Checkpoint saved at step 46.

 Training completed!
Best validation loss: 6.635021e-04
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (5000, 53, 53)
Test PSF images shape: (5000, 53, 53)
Test target images shape: (5000, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_superbit_normalized_low-noise46
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_superbit_normalized_low-noise46

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m2.847785e-03[0m
Mean Absolute Error (MAE): [1m[93m1.239266e-02[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m64.46 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.9886[0m
Approximate Perceptual Distance: [1m[96m5.004107e-03[0m
Bias: [1m+8.373823e-03[0m
Normalized MSE: [1m2.847785e-03[0m
Evaluation time: [1m[96m35.95 seconds[0m

[1m=== SANITY CHECKS ===[0m
[91mWARNING: PSNR > 50 dB is unusually high. Check for data leakage or model copying input.[0m
Prediction-Target Correlation: 0.998362

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m22.07 seconds[0m
Mean Squared Error (MSE): [1m[93m9.852582e-01[0m
Mean Absolute Error (MAE): [1m[93m2.471079e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m39.07 dB[0m
Bias: [1m+1.065168e-03[0m

==================================================
Method Comparison
==================================================
MSE                       2.848e-03       9.853e-01       [92mNeural[0m
MAE                       1.239e-02       2.471e-01       [92mNeural[0m
PSNR                      64.46           39.07           [92mNeural[0m
SSIM                      0.9886          0.1023          [92mNeural[0m
NORMALIZED_MSE            2.848e-03       9.853e-01       [92mNeural[0m
TIME_TAKEN                35.95s          22.07s          [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (5000, 53, 53)
GalSim predictions shape: (5000, 53, 53)
Target images shape: (5000, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_low-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 5000 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_low-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_superbit_normalized_low-noise

Evaluation complete!

Using config file: configs/deconvnet/research_backed/superbit_psf/not_normalized/high_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 0.01
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: False

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_superbit_not-normalized_low-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_superbit_not-normalized_low-noise/training_config.yaml

Epoch 1/300
  Training Loss: 2.331032e-05
  Validation Loss: 4.788084e-06
  New best validation loss: 4.788084e-06

Epoch 2/300
  Training Loss: 2.145381e-06
  Validation Loss: 3.414791e-06
  New best validation loss: 3.414791e-06

Epoch 3/300
  Training Loss: 1.732746e-06
  Validation Loss: 2.261165e-06
  New best validation loss: 2.261165e-06

Epoch 4/300
  Training Loss: 1.590216e-06
  Validation Loss: 2.083300e-06
  New best validation loss: 2.083300e-06

Epoch 5/300
  Training Loss: 1.514701e-06
  Validation Loss: 2.342905e-06
  No improvement. Patience: 1/25

Epoch 6/300
  Training Loss: 1.451960e-06
  Validation Loss: 2.373707e-06
  No improvement. Patience: 2/25

Epoch 7/300
  Training Loss: 1.403815e-06
  Validation Loss: 2.195330e-06
  No improvement. Patience: 3/25

Epoch 8/300
  Training Loss: 1.367465e-06
  Validation Loss: 1.735963e-06
  New best validation loss: 1.735963e-06

Epoch 9/300
  Training Loss: 1.334368e-06
  Validation Loss: 1.679237e-06
  New best validation loss: 1.679237e-06

Epoch 10/300
  Training Loss: 1.306320e-06
  Validation Loss: 1.958536e-06
  No improvement. Patience: 1/25

Epoch 11/300
  Training Loss: 1.286929e-06
  Validation Loss: 1.586986e-06
  New best validation loss: 1.586986e-06

Epoch 12/300
  Training Loss: 1.263939e-06
  Validation Loss: 1.668212e-06
  No improvement. Patience: 1/25

Epoch 13/300
  Training Loss: 1.249421e-06
  Validation Loss: 1.608944e-06
  No improvement. Patience: 2/25

Epoch 14/300
  Training Loss: 1.231471e-06
  Validation Loss: 1.394216e-06
  New best validation loss: 1.394216e-06

Epoch 15/300
  Training Loss: 1.214345e-06
  Validation Loss: 1.270922e-06
  New best validation loss: 1.270922e-06

Epoch 16/300
  Training Loss: 1.199513e-06
  Validation Loss: 1.287397e-06
  No improvement. Patience: 1/25

Epoch 17/300
  Training Loss: 1.181611e-06
  Validation Loss: 1.288788e-06
  No improvement. Patience: 2/25

Epoch 18/300
  Training Loss: 1.165625e-06
  Validation Loss: 1.307248e-06
  No improvement. Patience: 3/25

Epoch 19/300
  Training Loss: 1.151129e-06
  Validation Loss: 1.256039e-06
  New best validation loss: 1.256039e-06

Epoch 20/300
  Training Loss: 1.136953e-06
  Validation Loss: 1.454419e-06
  No improvement. Patience: 1/25

Epoch 21/300
  Training Loss: 1.121167e-06
  Validation Loss: 1.319902e-06
  No improvement. Patience: 2/25

Epoch 22/300
  Training Loss: 1.103438e-06
  Validation Loss: 1.189348e-06
  New best validation loss: 1.189348e-06

Epoch 23/300
  Training Loss: 1.091398e-06
  Validation Loss: 1.217540e-06
  No improvement. Patience: 1/25

Epoch 24/300
  Training Loss: 1.078138e-06
  Validation Loss: 1.125578e-06
  New best validation loss: 1.125578e-06

Epoch 25/300
  Training Loss: 1.066834e-06
  Validation Loss: 1.300228e-06
  No improvement. Patience: 1/25

Epoch 26/300
  Training Loss: 1.046328e-06
  Validation Loss: 1.297294e-06
  No improvement. Patience: 2/25

Epoch 27/300
  Training Loss: 1.027929e-06
  Validation Loss: 1.276385e-06
  No improvement. Patience: 3/25

Epoch 28/300
  Training Loss: 1.015595e-06
  Validation Loss: 1.265812e-06
  No improvement. Patience: 4/25

Epoch 29/300
  Training Loss: 9.954171e-07
  Validation Loss: 1.263854e-06
  No improvement. Patience: 5/25

Epoch 30/300
  Training Loss: 9.807328e-07
  Validation Loss: 1.252118e-06
  No improvement. Patience: 6/25

Epoch 31/300
  Training Loss: 9.658139e-07
  Validation Loss: 1.210429e-06
  No improvement. Patience: 7/25

Epoch 32/300
  Training Loss: 9.524408e-07
  Validation Loss: 1.314258e-06
  No improvement. Patience: 8/25

Epoch 33/300
  Training Loss: 9.381026e-07
  Validation Loss: 1.304402e-06
  No improvement. Patience: 9/25

Epoch 34/300
  Training Loss: 9.173299e-07
  Validation Loss: 1.200767e-06
  No improvement. Patience: 10/25

Epoch 35/300
  Training Loss: 9.012861e-07
  Validation Loss: 1.292949e-06
  No improvement. Patience: 11/25

Epoch 36/300
  Training Loss: 8.846530e-07
  Validation Loss: 1.331874e-06
  No improvement. Patience: 12/25

Epoch 37/300
  Training Loss: 8.678297e-07
  Validation Loss: 1.378763e-06
  No improvement. Patience: 13/25

Epoch 38/300
  Training Loss: 8.490245e-07
  Validation Loss: 1.194351e-06
  No improvement. Patience: 14/25

Epoch 39/300
  Training Loss: 8.339722e-07
  Validation Loss: 1.222052e-06
  No improvement. Patience: 15/25

Epoch 40/300
  Training Loss: 8.154862e-07
  Validation Loss: 1.215577e-06
  No improvement. Patience: 16/25

Epoch 41/300
  Training Loss: 8.007162e-07
  Validation Loss: 1.347181e-06
  No improvement. Patience: 17/25

Epoch 42/300
  Training Loss: 7.862348e-07
  Validation Loss: 1.225867e-06
  No improvement. Patience: 18/25

Epoch 43/300
  Training Loss: 7.696006e-07
  Validation Loss: 1.336370e-06
  No improvement. Patience: 19/25

Epoch 44/300
  Training Loss: 7.536972e-07
  Validation Loss: 1.282016e-06
  No improvement. Patience: 20/25

Epoch 45/300
  Training Loss: 7.379467e-07
  Validation Loss: 1.170508e-06
  No improvement. Patience: 21/25

Epoch 46/300
  Training Loss: 7.241395e-07
  Validation Loss: 1.276432e-06
  No improvement. Patience: 22/25

Epoch 47/300
  Training Loss: 7.042185e-07
  Validation Loss: 1.338760e-06
  No improvement. Patience: 23/25

Epoch 48/300
  Training Loss: 6.871746e-07
  Validation Loss: 1.283007e-06
  No improvement. Patience: 24/25

Epoch 49/300
  Training Loss: 6.714015e-07
  Validation Loss: 1.492056e-06
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 49
Checkpoint saved at step 49.

 Training completed!
Best validation loss: 1.125578e-06
Plotting learning curve...
Saving training and validation loss...

Using config file: configs/deconvnet/research_backed/superbit_psf/not_normalized/low_noise.yaml

==================================================
Training Configuration
==================================================

dataset:
  samples: 100000
  psf_sigma: 0.25
  exp: superbit
  nse_sd: 1e-05
  seed: 42
  stamp_size: 53
  pixel_size: 0.141
  apply_psf_shear: True
  psf_shear_range: 0.05
  normalized: False

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

training:
  epochs: 300
  batch_size: 8
  learning_rate: 0.001
  weight_decay: 0.0001
  patience: 25
  val_split: 0.2
  eval_interval: 1

output:
  save_path: /home/adfield/ShearNet/model_checkpoint
  plot_path: /home/adfield/ShearNet/plots
  model_name: research_backed_superbit_not-normalized_low-noise

plotting:
  plot: True
  num_plot_samples: 10
==================================================

Running on device: cuda:0
Generating deconvolution dataset...
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 16424 x 16424, which requires 6.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17946 x 17946, which requires 7.20 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 17064 x 17064, which requires 6.51 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 11584 x 11584, which requires 3.00 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 20362 x 20362, which requires 9.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 26980 x 26980, which requires 16.27 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 9260 x 9260, which requires 1.92 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
GalSim error drawing clean galaxy: drawFFT requires an FFT that is too large.
The required FFT size would be 25054 x 25054, which requires 14.03 GB of memory.
If you can handle the large FFT, you may update gsparams.maximum_fft_size.
Galaxy images shape: (100000, 53, 53)
PSF images shape: (100000, 53, 53)
Target images shape: (100000, 53, 53)

Training configuration saved to: /home/adfield/ShearNet/plots/research_backed_superbit_not-normalized_low-noise/training_config.yaml

Epoch 1/300
  Training Loss: 1.880586e-05
  Validation Loss: 5.394235e-07
  New best validation loss: 5.394235e-07

Epoch 2/300
  Training Loss: 4.483252e-07
  Validation Loss: 2.882921e-07
  New best validation loss: 2.882921e-07

Epoch 3/300
  Training Loss: 2.753445e-07
  Validation Loss: 2.072479e-07
  New best validation loss: 2.072479e-07

Epoch 4/300
  Training Loss: 2.081133e-07
  Validation Loss: 2.621289e-07
  No improvement. Patience: 1/25

Epoch 5/300
  Training Loss: 1.788166e-07
  Validation Loss: 3.186038e-07
  No improvement. Patience: 2/25

Epoch 6/300
  Training Loss: 1.534964e-07
  Validation Loss: 3.170635e-07
  No improvement. Patience: 3/25

Epoch 7/300
  Training Loss: 1.473498e-07
  Validation Loss: 2.260826e-07
  No improvement. Patience: 4/25

Epoch 8/300
  Training Loss: 1.273341e-07
  Validation Loss: 7.616238e-07
  No improvement. Patience: 5/25

Epoch 9/300
  Training Loss: 1.150098e-07
  Validation Loss: 1.890805e-07
  New best validation loss: 1.890805e-07

Epoch 10/300
  Training Loss: 1.085803e-07
  Validation Loss: 3.023508e-07
  No improvement. Patience: 1/25

Epoch 11/300
  Training Loss: 9.847955e-08
  Validation Loss: 2.683076e-07
  No improvement. Patience: 2/25

Epoch 12/300
  Training Loss: 9.395815e-08
  Validation Loss: 4.469490e-07
  No improvement. Patience: 3/25

Epoch 13/300
  Training Loss: 8.993849e-08
  Validation Loss: 2.782402e-07
  No improvement. Patience: 4/25

Epoch 14/300
  Training Loss: 8.504018e-08
  Validation Loss: 4.218097e-07
  No improvement. Patience: 5/25

Epoch 15/300
  Training Loss: 7.991936e-08
  Validation Loss: 4.308111e-07
  No improvement. Patience: 6/25

Epoch 16/300
  Training Loss: 7.617898e-08
  Validation Loss: 4.289227e-07
  No improvement. Patience: 7/25

Epoch 17/300
  Training Loss: 7.539624e-08
  Validation Loss: 2.579349e-07
  No improvement. Patience: 8/25

Epoch 18/300
  Training Loss: 7.410421e-08
  Validation Loss: 3.444296e-07
  No improvement. Patience: 9/25

Epoch 19/300
  Training Loss: 6.846733e-08
  Validation Loss: 3.190309e-07
  No improvement. Patience: 10/25

Epoch 20/300
  Training Loss: 6.788642e-08
  Validation Loss: 4.862763e-07
  No improvement. Patience: 11/25

Epoch 21/300
  Training Loss: 6.464806e-08
  Validation Loss: 3.984715e-07
  No improvement. Patience: 12/25

Epoch 22/300
  Training Loss: 6.215788e-08
  Validation Loss: 2.842741e-07
  No improvement. Patience: 13/25

Epoch 23/300
  Training Loss: 6.111406e-08
  Validation Loss: 4.394266e-07
  No improvement. Patience: 14/25

Epoch 24/300
  Training Loss: 5.946974e-08
  Validation Loss: 3.537375e-07
  No improvement. Patience: 15/25

Epoch 25/300
  Training Loss: 5.640680e-08
  Validation Loss: 4.945014e-07
  No improvement. Patience: 16/25

Epoch 26/300
  Training Loss: 5.615394e-08
  Validation Loss: 4.614131e-07
  No improvement. Patience: 17/25

Epoch 27/300
  Training Loss: 5.100357e-08
  Validation Loss: 4.335751e-07
  No improvement. Patience: 18/25

Epoch 28/300
  Training Loss: 5.135171e-08
  Validation Loss: 4.995990e-07
  No improvement. Patience: 19/25

Epoch 29/300
  Training Loss: 4.887147e-08
  Validation Loss: 4.873110e-07
  No improvement. Patience: 20/25

Epoch 30/300
  Training Loss: 4.830192e-08
  Validation Loss: 4.735580e-07
  No improvement. Patience: 21/25

Epoch 31/300
  Training Loss: 4.616814e-08
  Validation Loss: 6.100657e-07
  No improvement. Patience: 22/25

Epoch 32/300
  Training Loss: 4.546603e-08
  Validation Loss: 5.668612e-07
  No improvement. Patience: 23/25

Epoch 33/300
  Training Loss: 4.292366e-08
  Validation Loss: 6.397943e-07
  No improvement. Patience: 24/25

Epoch 34/300
  Training Loss: 4.278587e-08
  Validation Loss: 6.494731e-07
  No improvement. Patience: 25/25

 Early stopping triggered at epoch 34
Checkpoint saved at step 34.

 Training completed!
Best validation loss: 1.890805e-07
Plotting learning curve...
Saving training and validation loss...

Loading model config from: /home/adfield/ShearNet/plots/research_backed_superbit_not-normalized_low-noise/training_config.yaml

==================================================
Evaluation Configuration
==================================================

evaluation:
  test_samples: 5000
  seed: 58

model:
  process_psf: False
  type: cnn
  galaxy: {'type': 'research_backed'}
  psf: {'type': 'forklens_psf'}

plotting:
  plot: True
  num_plot_samples: 10

comparison:
  mcal: True
  ngmix: True
  psf_model: gauss
  gal_model: gauss
==================================================


Generating test dataset...
Test galaxy images shape: (5000, 53, 53)
Test PSF images shape: (5000, 53, 53)
Test target images shape: (5000, 53, 53)
Loading checkpoint from directory: /home/adfield/ShearNet/model_checkpoint/research_backed_superbit_not-normalized_low-noise34
Warming up model...
Model warm-up complete
Successfully loaded checkpoint from /home/adfield/ShearNet/model_checkpoint/research_backed_superbit_not-normalized_low-noise34

==================================================
Evaluating Neural Deconvolution Model
==================================================
Compiling evaluation function...
Compilation complete. Running evaluation...

[1m=== Neural Deconvolution Results ===[0m
Mean Squared Error (MSE): [1m[93m2.185524e-01[0m
Mean Absolute Error (MAE): [1m[93m1.685856e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m45.49 dB[0m
Structural Similarity Index (SSIM): [1m[96m0.8395[0m
Approximate Perceptual Distance: [1m[96m3.257963e-01[0m
Bias: [1m-4.415079e-05[0m
Normalized MSE: [1m2.185524e-01[0m
Evaluation time: [1m[96m35.96 seconds[0m

[1m=== SANITY CHECKS ===[0m
Prediction-Target Correlation: 0.890819

==================================================
Evaluating GalSim Deconvolution
==================================================

[1m=== GalSim Deconvolution (Using Pre-existing Obs) ===[0m
Evaluation Time: [1m[96m21.87 seconds[0m
Mean Squared Error (MSE): [1m[93m9.851486e-01[0m
Mean Absolute Error (MAE): [1m[93m2.483023e-01[0m
Peak Signal-to-Noise Ratio (PSNR): [1m[96m38.96 dB[0m
Bias: [1m+1.077607e-03[0m

==================================================
Method Comparison
==================================================
MSE                       2.186e-01       9.851e-01       [92mNeural[0m
MAE                       1.686e-01       2.483e-01       [92mNeural[0m
PSNR                      45.49           38.96           [92mNeural[0m
SSIM                      0.8395          0.1035          [92mNeural[0m
NORMALIZED_MSE            2.186e-01       9.851e-01       [92mNeural[0m
TIME_TAKEN                35.96s          21.87s          [91mGalSim[0m

======================================================================
[1m[92mOverall Winner: Neural Network Deconvolution[0m
Neural network wins 5 out of 5 key metrics

Generating evaluation plots...
Generating neural network predictions...
Neural predictions shape: (5000, 53, 53)
GalSim predictions shape: (5000, 53, 53)
Target images shape: (5000, 53, 53)
Creating comparison plot...
Comparison plot saved to: /home/adfield/ShearNet/plots/research_backed_superbit_not-normalized_low-noise/comparison.png
Creating spatial residuals heat map...
Computing spatial residuals for 5000 images...
Spatial residuals plot saved to: /home/adfield/ShearNet/plots/research_backed_superbit_not-normalized_low-noise/spatial_residual.png
Plots saved to: /home/adfield/ShearNet/plots/research_backed_superbit_not-normalized_low-noise

Evaluation complete!
